[
  {
    "objectID": "Concluding_Thoughts.html",
    "href": "Concluding_Thoughts.html",
    "title": "Conclusion",
    "section": "",
    "text": "In this project, we aimed to predict assault incidents across Chicago using a Poisson regression model. Our approach began with the systematic retrieval of urban risk factor data, such as graffiti, non-functional street lights, liquor retail stores, and ShotSpotter incidents. After preparing and aggregating the data by spatially joining it with a fishnet grid, we performed a series of analyses. We calculated the number of occurrences of each risk factor within the grid cells and merged these counts with the assault data. Using these aggregated risk factors, we trained a Poisson regression model to predict assault counts across the city. The model results were then visualized through a choropleth map, which highlighted the predicted assault counts and revealed several hot spots, particularly in South Chicago, with potentially two other significant areas in Central Chicago. Residual analysis further revealed areas where the model either overpredicted or underpredicted assaults, with blue areas indicating severe underpredictions and dark red areas showing overpredictions. Finally, we examined the correlation between predictor variables, which generally exhibited low to moderate correlations, confirming that the risk factors were relatively independent of one another.\nWhile the methodology employed in this project was effective in predicting assault patterns across Chicago, there are several areas where improvements or adjustments could be made to enhance the model’s accuracy and predictive power. First, while the Poisson regression model is suitable for count data, it assumes that the variance equals the mean, which may not hold in all cases. Exploring alternative models, such as the Negative Binomial regression, could potentially improve performance in areas where overdispersion is present, as it accounts for greater variability in the data. Additionally, the inclusion of more granular or additional risk factors—such as socioeconomic indicators, police presence, or neighborhood-level interventions—could help refine the model and capture underlying causes of crime more accurately. Another area of improvement lies in the spatial resolution of the analysis. While using a fishnet grid was effective for aggregating data, further exploration of different spatial units, such as census tracts or neighborhood boundaries, could provide a more nuanced view of crime patterns. Lastly, addressing potential model overfitting by incorporating cross-validation techniques could help ensure the model generalizes better to unseen data, improving its robustness and predictive power. These adjustments would enhance the model’s ability to offer more precise and actionable insights for crime prevention and public policy.\n\n\n\n Back to top"
  },
  {
    "objectID": "AnalysisResults/CorMatrix.html",
    "href": "AnalysisResults/CorMatrix.html",
    "title": "Correlation Matrix",
    "section": "",
    "text": "To further evaluate the relationships among the predictors used in the model, we computed a correlation matrix and visualized it using a heatmap. The heatmap provides a detailed view of the pairwise correlations between all predictors, with values ranging from -1 to 1. Strong positive correlations are represented in shades of green, while strong negative correlations appear in shades of purple. This analysis helps identify potential multicollinearity among variables, which could impact the model’s reliability and interpretability. By examining the strength and direction of these correlations, we can determine whether any predictors might need to be adjusted, combined, or removed in future modeling efforts.\nThe heatmap of the correlation matrix revealed that the majority of the variables exhibited moderate to low correlation values. This suggests that there is limited multicollinearity among the predictors, indicating that each variable contributes relatively distinct information to the model.\n\n\nCode\ncorrelation_matrix = features.corr()\n\n# Plot heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap=cmap, fmt='.2f')\nplt.title('Correlation Matrix of Predictors', fontsize=20, fontweight='bold')\nplt.xticks(rotation=90, ha='center', fontsize=8)\nplt.yticks(fontsize=8)\nplt.show()\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "AnalysisResults/Density.html",
    "href": "AnalysisResults/Density.html",
    "title": "Density Map of Assaults",
    "section": "",
    "text": "Following the creation of the Assault21 data frame, the density of assault incidents in Chicago is estimated using Kernel Density Estimation (KDE). First, the coordinates of the assault locations are extracted from the Assault21 GeoDataFrame into a NumPy array, ensuring that any NaN values are removed. KDE is then applied to these coordinates with a Gaussian kernel and a bandwidth of 0.3 to smooth the density estimation. The boundaries of Chicago are retrieved using chicagoBoundary.total_bounds, and a grid of positions (x and y values) is created over the city’s spatial extent. The KDE model is then evaluated over this grid to calculate the density of assault incidents at each point.\nThe density map reveals four clear hot spots of assault incidents across Chicago, concentrated in the central, northeast, and southern areas of the city. These regions exhibit higher concentrations of assaults, indicated by the denser areas in red on the map. The central area, often associated with downtown and surrounding neighborhoods, shows a significant cluster of incidents, while the northeastern part and the southern tip of the city also feature notable concentrations. These hot spots highlight areas that may warrant further investigation or targeted interventions to address the higher frequency of assault cases in these locations.\n\n\nCode\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ncolors4 = ['#27232e', '#777181', '#d0c7e1', '#e1e0be', '#c2e538'] \ncmap4 = LinearSegmentedColormap.from_list('custom_gradient', colors4, N=256)\n\ncoords = np.vstack((Assault21.geometry.x, Assault21.geometry.y)).T\n\nplt.figure(figsize=(10, 10))\nchicagoBoundary.to_crs(4326).plot(ax=plt.gca(), color='black', edgecolor='black', linewidth=1)\nsns.kdeplot(x=coords[:, 0], y=coords[:, 1], fill=True, cmap=cmap4, bw_adjust=0.5)\n\nplt.title(\"Heatmap of Assault Incidents in Chicago 2021\")\nplt.xticks([])\nplt.yticks([])\nplt.gca().set_facecolor('white')\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['left'].set_visible(False)\nplt.gca().spines['bottom'].set_visible(False)\nplt.show()\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "AnalysisResults/VisualizingRiskFactors.html",
    "href": "AnalysisResults/VisualizingRiskFactors.html",
    "title": "Visualizing Risk Factors",
    "section": "",
    "text": "To gain a better understanding of the spatial distribution of various risk factors across Chicago, we visualized the locations of key variables, graffiti, non-functional street lights, liquor retail stores, and ShotSpotter technology—within the city’s grid. These risk factors serve as proxies for urban challenges that might correlate with patterns of criminal activity or community issues.\nWe used a fishnet grid, derived from the geometry of the city’s boundaries, to aggregate the data and visualize the spatial distribution of these variables across distinct geographic areas. The maps below show how each risk factor is distributed within the city:\n\nGraffiti Removal Requests: Requests for graffiti removal are primarily concentrated in the city center, reflecting urbanization and possibly higher rates of vandalism in more densely populated areas.\nLiquor Retail Stores: Liquor retail stores are predominantly located in the northeastern part of the city, suggesting potential correlations with higher consumption rates in these areas.\nShotSpotter Technology: ShotSpotter sensors, used to detect gunfire, are mainly concentrated in the northwestern and southern parts of the city, highlighting areas where gun violence has been a concern.\nNon-functional Street Lights: Complaints about street lights being out are widespread across Chicago, indicating that lighting infrastructure issues affect multiple neighborhoods, potentially contributing to safety concerns.\n\n\n\nCode\ngraffiti = client.get(\"hec5-y4x5\", limit=1000000)\n\ngraffiti = pd.DataFrame.from_records(graffiti)\n\n\n\ngraffiti['year'] = graffiti['creation_date'].str[:4]\ngraffiti = graffiti[graffiti['year'] == \"2018\"]\ngraffiti = graffiti[graffiti['where_is_the_graffiti_located_'].isin([\"Front\", \"Rear\", \"Side\"])]\ngraffiti = graffiti[['latitude', 'longitude']].dropna()\n\ngraffiti = gpd.GeoDataFrame(\n    graffiti, \n    geometry=gpd.points_from_xy(graffiti.longitude, graffiti.latitude),\n    crs=\"EPSG:4326\"\n)\n\ngraffiti = graffiti.to_crs(fishnet.crs)\ngraffiti['Legend'] = \"Graffiti\"\n\n\n\nstreetLightsOut = client.get(\"zuxi-7xem\", limit=1000000)\n\nstreetLightsOut = pd.DataFrame.from_records(streetLightsOut)\n\n\n\nstreetLightsOut['year'] = streetLightsOut['creation_date'].str[:4]\nstreetLightsOut = streetLightsOut[streetLightsOut['year'] == \"2018\"]\nstreetLightsOut = streetLightsOut[['latitude', 'longitude']].dropna()\nstreetLightsOut = gpd.GeoDataFrame(streetLightsOut, geometry=gpd.points_from_xy(streetLightsOut.longitude, streetLightsOut.latitude), crs=\"EPSG:4326\")\nstreetLightsOut = streetLightsOut.to_crs(fishnet.crs)\nstreetLightsOut['Legend'] = \"streetLightsOut\"\n\n\n\nliquorRetail = client.get(\"nrmj-3kcf\", limit=1000000)\n\nliquorRetail = pd.DataFrame.from_records(liquorRetail)\n\n\n\nliquorRetail = liquorRetail[liquorRetail['business_activity'] == \"Retail Sales of Packaged Liquor\"]\nliquorRetail = liquorRetail[['latitude', 'longitude']].dropna()\nliquorRetail = gpd.GeoDataFrame(liquorRetail, geometry=gpd.points_from_xy(liquorRetail.longitude, liquorRetail.latitude), crs=\"EPSG:4326\")\nliquorRetail = liquorRetail.to_crs(fishnet.crs)\nliquorRetail['Legend'] = \"liquorRetail\"\n\n\n\nshotSpotter = client.get(\"3h7q-7mdb\", limit=1000000)\n\nshotSpotter = pd.DataFrame.from_records(shotSpotter)\n\n\n\nshotSpotter = shotSpotter[['latitude', 'longitude']].dropna()\nshotSpotter = gpd.GeoDataFrame(shotSpotter, geometry=gpd.points_from_xy(shotSpotter.longitude, shotSpotter.latitude), crs=\"EPSG:4326\")\nshotSpotter = shotSpotter.to_crs(fishnet.crs)\nshotSpotter['Legend'] = \"shotSpotter\"\n\n\n\nvariable_net = pd.concat([streetLightsOut, liquorRetail, graffiti, shotSpotter]) \\\n                .sjoin(fishnet, how=\"inner\", predicate='within') #\\\n\n\nvariable_net = gpd.GeoDataFrame(variable_net, geometry='geometry', crs=\"EPSG:4326\")\n\n#variable_net\n\n\n\nRisk Factor Distribution Maps\nTo visualize the spatial patterns of these risk factors, we generated a series of maps displaying the distribution of each variable. Each risk factor was plotted separately, allowing us to examine the geographic concentration of the different factors across the city. Below is the process used to create these maps:\n\nData Processing: Each dataset was cleaned and filtered to ensure it represented data from 2018. The datasets were then converted to the same coordinate reference system (CRS) as the fishnet grid to allow for spatial joins.\nSubsetting Data by Risk Factor: We extracted the individual risk factors (graffiti, street lights, liquor retail stores, and ShotSpotter) and created subsets based on their “Legend” column. Each subset corresponds to a specific risk factor.\nMaps: For each subset, a separate subplot was generated, displaying the locations of the risk factor points. The following visualization details the distribution of each risk factor:\n\nEach point represents a location within the city that corresponds to one of the risk factors.\nThe points are displayed with a uniform color (green) and size for consistency across the maps.\nThe aspect ratio for each map was set to “equal” to ensure proportionality across the different risk factor maps.\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\n\nlegend_values = variable_net['Legend'].unique()\n\n# Create a GridSpec with a tighter layout\nfig = plt.figure(figsize=(20, 5))\ngs = GridSpec(1, len(legend_values), figure=fig, wspace=0.1)\n\nfor i, legend_value in enumerate(legend_values):\n    ax = fig.add_subplot(gs[i])\n    subset = variable_net[variable_net['Legend'] == legend_value]\n    chicagoBoundary.plot(ax=ax, color='black', edgecolor='black')  # Plot the boundary first\n    subset.plot(ax=ax, marker='o', color='#c2e538', markersize=10, alpha=0.5, aspect=1)\n    ax.set_title(legend_value)  # Removed \"Legend: \" prefix\n    ax.set_aspect('equal')  # Set aspect to 'equal' manually\n    ax.set_xticklabels(ax.get_xticks(), fontsize=8)  # Make x-axis labels smaller\n    ax.set_yticklabels(ax.get_yticks(), fontsize=8)  # Make y-axis labels smaller\n    ax.axis('on')  # Keep axis visible for better debugging\n\nplt.tight_layout()\nplt.show()\n\n\n\nRequests for graffiti removal are primarily concentrated in the city center, while liquor retail stores are predominantly found in the northeastern part of the city. In contrast, ShotSpotter technology is mainly located in the northwestern and southern areas of Chicago. Notably, complaints about non-functioning street lights are spread throughout the entire city, highlighting a widespread issue that affects various neighborhoods.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "AnalysisResults/LocalSpatialAutocorrelation.html",
    "href": "AnalysisResults/LocalSpatialAutocorrelation.html",
    "title": "Local Spatial Autocorrelation",
    "section": "",
    "text": "To gain a deeper understanding of the spatial patterns associated with assault incidents in Chicago, we focused on pinpointing assault hotspots. While the global Moran’s I statistic assesses overall spatial autocorrelation across the entire area, Local Moran’s I allows us to evaluate the clustering of assault incidents at specific locations, determining whether assault counts are randomly distributed relative to their neighboring areas.\nFor this analysis, we created a spatial weights matrix based on queen contiguity, which connects each grid cell to its eight immediate neighbors. This matrix was essential for identifying spatial dependencies in the assault data.\n\nSpatial Weights Matrix: We used the Queen.from_dataframe method to construct a spatial weights matrix for the assault data, which connects each grid cell to its eight neighboring cells based on their contiguity.\nLocal Moran’s I Calculation:\n\nThe Moran_Local function was applied to calculate the Local Moran’s I values for each grid cell in relation to its neighbors. This test evaluates whether the assault counts in each grid cell are spatially correlated with those in adjacent cells.\n\nCluster Analysis:\n\nThe output of the Local Moran’s I test included a Cluster Label for each grid cell, indicating whether it exhibited High-High (HH), Low-High (LH), Low-Low (LL), or High-Low (HL) spatial patterns based on the values of Local Moran’s I and their statistical significance.\nCluster Labels were assigned as follows:\n\nHH (High-High): High assault counts surrounded by other high assault counts.\nLH (Low-High): Low assault counts surrounded by high assault counts.\nLL (Low-Low): Low assault counts surrounded by other low assault counts.\nHL (High-Low): High assault counts surrounded by low assault counts.\n\n\nVisualization:\n\nThe results of the Local Moran’s I analysis were visualized by plotting the clusters on the map, with distinct colors assigned to each cluster type. This allowed us to clearly highlight areas with significant spatial autocorrelation, which are potential assault hotspots.\n\n\n\n\n\n\nCode\n!pip install esda\n\nfrom libpysal.weights import Queen\nfrom esda.moran import Moran_Local\n\nw = Queen.from_dataframe(Assault21_net, use_index = False)\ny = Assault21_net['countAssault']\nmoran_loc = Moran_Local(y, w)\n\nAssault21_net['LocalMoran'] = moran_loc.Is\nAssault21_net['Cluster'] = moran_loc.q\ncluster_labels = {\n    1: \"HH (High-High)\",\n    2: \"LH (Low-High)\",\n    3: \"LL (Low-Low)\",\n    4: \"HL (High-Low)\"\n}\nAssault21_net['ClusterLabel'] = Assault21_net['Cluster'].map(cluster_labels)\n\ncluster_colors2 = ['#c2e538', '#d0c7e1', '#777181', '#27232e']  # Example colors\ncmap2 = ListedColormap(cluster_colors2)\n\n# Plot\nfig, ax = plt.subplots(1, 1, figsize=(8, 8))\nAssault21_net.plot(column='ClusterLabel', categorical=True, legend=True, cmap=cmap2, ax=ax, legend_kwds={'fontsize': 8})\nplt.title(\"Local Moran's I Clusters\", fontsize=15)\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['left'].set_visible(False)\nplt.gca().spines['bottom'].set_visible(False)\nplt.xticks([])\nplt.yticks([])\nplt.show()\n\n\nThe analysis revealed significant hotspots—defined as grid cells where the local assault counts exceeded the expected values with a p-value ≤ 0.05. These hotspots, particularly concentrated in central to southern Chicago, indicate areas with a strong spatial correlation of assault incidents among neighboring grid cells. These findings align with patterns observed in the kernel density map, further confirming the areas where interventions may be most needed.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "AnalysisResults/Fishnet.html",
    "href": "AnalysisResults/Fishnet.html",
    "title": "Fishnet Grid",
    "section": "",
    "text": "To accurately capture the spatial distribution of assault incidents, a fishnet grid is generated over the study area. This grid divides the region into uniform, equal-sized cells, allowing for a consistent spatial analysis that avoids potential biases introduced by irregular geographic boundaries such as neighborhoods. By using a fishnet grid, the analysis can achieve a finer spatial resolution, enabling the detection of localized assault patterns that might be overlooked when working with larger areas. Additionally, the grid allows for data aggregation within each cell, smoothing out any noise from individual incidents and highlighting broader spatial trends.\nThe code provided defines a function, create_fishnet(), which generates the fishnet grid by first determining the bounding box of the study area, then creating a series of square cells of a specified size (500 meters in this case). The function also ensures that the grid only includes cells that intersect with the study area’s boundary. The resulting grid is then plotted, with each cell shaded in a light (purple) color, providing a visual representation of the spatial structure used for further analysis. This approach improves the accuracy of subsequent spatial modeling techniques, such as regression and density estimation, by standardizing the area for analysis.\n\n\nCode\ndef create_fishnet(boundary, cellsize):\n    bounds = boundary.total_bounds  # Get bounding box of the area\n    xmin, ymin, xmax, ymax = bounds\n    cols = np.arange(xmin, xmax + cellsize, cellsize)\n    rows = np.arange(ymin, ymax + cellsize, cellsize)\n    polygons = [\n        box(x, y, x + cellsize, y + cellsize)\n        for x in cols[:-1] for y in rows[:-1]\n    ]\n    fishnet = gpd.GeoDataFrame(geometry=polygons, crs=boundary.crs)\n    # Intersect with boundary\n    fishnet = fishnet[fishnet.intersects(boundary.unary_union)]\n    fishnet[\"uniqueID\"] = range(1, len(fishnet) + 1)\n    return fishnet\n\n# Generate fishnet\nfishnet = create_fishnet(chicagoBoundary, cellsize=2500)\n\n# Plot fishnet\nfig, ax = plt.subplots(figsize=(10, 10))\nfishnet.plot(ax=ax, edgecolor=\"white\", facecolor=\"#d0c7e1\")\nchicagoBoundary.boundary.plot(ax=ax, color=\"#777181\", linewidth=2.25)\nax.set_title(\"Fishnet of Chicago\", fontsize=20, fontweight=\"bold\")\nax.axis(\"off\")\nplt.show()\n\n\n\n\nAfterwards, to analyze the distribution of assaults across the fishnet grid, a new column, countAssault, is created in the Assault21 DataFrame, where each assault incident is assigned a value of 1. A spatial join is then performed using the gpd.sjoin() function to map each assault incident to its corresponding grid cell based on location. The spatial join is done using the contains operator, which ensures that each assault is assigned to the fishnet cell that contains its point coordinates. This operation results in a new DataFrame, Assault21_net, which aggregates the count of assault incidents per fishnet grid cell by grouping the data by the unique grid cell ID (uniqueID). The final output includes the sum of assaults (countAssault) for each grid cell along with the grid cell geometry.\nThe resulting DataFrame, Assault21_net, contains the total count of assaults within each fishnet cell, allowing for a more structured and standardized approach to understanding spatial trends. This transformation enables the analysis of assault patterns on a finer spatial scale, avoiding the biases introduced by irregular neighborhood boundaries. The addition of a countAssault column allows for efficient data aggregation, which is essential for performing further spatial analyses, such as identifying hotspots or correlating assault density with other geographic factors.\n\n\nCode\n# Count of Assaults in each fishnet grid\nAssault21[\"countAssault\"] = 1\nAssault21_net = gpd.sjoin(fishnet, Assault21.to_crs(3435), how=\"left\", op=\"contains\")\nAssault21_net = Assault21_net.groupby(\"uniqueID\").agg(\n    countAssault=(\"countAssault\", \"sum\"),\n    geometry=(\"geometry\", \"first\")\n).reset_index()\n\n#Assault21\n\n\nIn the following steps, the count of assaults (countAssault) in the Assault21_net DataFrame is processed further to handle any missing values. Specifically, the fillna(0) method is used to replace any NaN values in the countAssault column with 0. This ensures that grid cells without any assaults are represented with a count of 0, making the dataset complete and suitable for analysis.\nAdditionally, two new columns are added to Assault21_net:\n\nuniqueID: A unique identifier is created for each grid cell, ranging from 1 to the total number of grid cells. This ID is useful for distinguishing each fishnet grid cell and will be helpful in further analyses, such as regression modeling or spatial clustering.\ncvID: A cross-validation ID (cvID) is generated for each row using the np.random.randint function. The values of cvID are randomly assigned, ensuring that they fall within a range of integers, distributed approximately evenly across all grid cells. The size of each group is set to be around one twenty-fourth of the total number of grid cells, which is typical for cross-validation purposes.\n\n\n\nCode\nAssault21.crs\n\nAssault21_net[\"countAssault\"] = Assault21_net[\"countAssault\"].fillna(0)\n\n# Add uniqueID and cross-validation IDs\nAssault21_net[\"uniqueID\"] = range(1, len(Assault21_net) + 1)\nAssault21_net[\"cvID\"] = np.random.randint(\n    low=1, high=(len(Assault21_net) // 24) + 1, size=len(Assault21_net)\n)\n\n#Assault21_net\n\n\nWe use the fishnet to then visualize our assault cases again to gain a more accurate picture of the number of cases relative to the area in its given grid. The map shows a few grids with higher assault counts in the north east and south.\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\nimport geopandas as gpd\n\n# Define custom gradient\ncolors = ['#27232e', '#777181', '#d0c7e1', '#e1e0be', '#c2e538']\ncmap = LinearSegmentedColormap.from_list('custom_gradient', colors, N=256)\n\n# Plot Assault Counts\nfig, ax = plt.subplots(figsize=(8, 8))\nAssault21_net = gpd.GeoDataFrame(Assault21_net, geometry='geometry')  # Ensure 'geometry' column exists\nAssault21_net.plot(column='countAssault', ax=ax, legend=False, cmap=cmap)\n\nplt.title(\"Count of Assaults (Fishnet)\", fontsize=20, fontweight='bold')\nplt.axis('off')\nplt.gca().set_xticklabels([])\nplt.gca().set_yticklabels([])\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['left'].set_visible(False)\nplt.gca().spines['bottom'].set_visible(False)\n\ncax = fig.add_axes([0.15, 0.1, 0.7, 0.01]) # Adjust position and size \ncb = plt.colorbar(plot.get_children()[0], cax=cax, orientation='horizontal') \ncb.set_label(\"Assault Counts\")\ncb.ax.tick_params(labelsize=8)\n\nplt.show()\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "AnalysisResults/Data_Gather.html",
    "href": "AnalysisResults/Data_Gather.html",
    "title": "Assault Data (Data Gathering)",
    "section": "",
    "text": "Our data is sourced from the Chicago Data Portal, which provides a variety of municipal datasets for public use. For our model, we specifically use assault case data from 2021. While our original goal was to analyze assault cases from 2020 to 2023 to explore patterns during the COVID-19 pandemic, for the sake of simplicity and time constraints, we have chosen 2021 as a representative year. We intend to later predict assault cases for 2022 and assess the accuracy of our model.\n\n\nCode\n!pip install census\n!pip install us\n!pip install sodapy\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport geopandas as gpd\nfrom census import Census\nfrom us import states\nimport os\nfrom sodapy import Socrata\nfrom shapely.geometry import Point\nimport numpy as np\nfrom scipy.stats import gaussian_kde\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom sklearn.neighbors import KernelDensity\nfrom shapely.geometry import box\nfrom matplotlib.colors import Normalize\nfrom matplotlib.colorbar import ColorbarBase\nimport seaborn as sns\n\n\nTo analyze crime patterns in Chicago, we began by retrieving data from the city’s open data portal using the Socrata API. The Socrata API provides an efficient method for accessing large public datasets. Below is an overview of the steps involved in the data retrieval process:\n\nData Retrieval: We obtained the dataset containing crime reports (dataset ID: dwme-t96c), specifying a limit of 210,000 records to ensure a comprehensive dataset for analysis. This dataset includes key details such as the date, location, crime type, and other relevant contextual information about each incident.\nConversion to Pandas DataFrame: The data returned from the API is in JSON format, which was then converted into a Pandas DataFrame. This transformation makes the data easier to manipulate and analyze, using Pandas’ powerful tools for data cleaning, filtering, and exploration.  \nData Structure: The dataset contains several columns, including:\n\nDate and Location: Timestamp and coordinates of each incident.\nCrime Descriptions: Categorized details about the crime, including the type of crime and location (e.g., street address, community area).\nAdministrative Details: Information about police districts, case numbers, and arrest status.\n\nSetup: This initial step ensures we have a clean and manageable dataset for analysis. The extracted data serves as the foundation for preprocessing, feature engineering, and model development in the subsequent stages of the project.\n\n\n\nCode\nclient = Socrata(\"data.cityofchicago.org\",\n                  \"PXGs3LAGSv2IZaGVJVPf1M0Fz\",\n                  username=\"jijinc@upenn.edu\",\n                  password=\"8m9reD@XfA$Z5W.\")\n\nresults = client.get(\"dwme-t96c\", limit=210000)\n\nresults_df = pd.DataFrame.from_records(results)\n\n\nAfter these steps, the dataset (results_df) was filtered to extract only rows where the primary_type is “ASSAULT”, creating a new DataFrame named assault. A new column, geometry, was added by converting the longitude and latitude values into Point objects using the shapely.geometry library. This transformation allows for spatial representation of assault incidents.\nThe filtered dataset was then converted into a GeoDataFrame called Assault21, a specialized Pandas object that supports spatial operations. The geometry column specifies the spatial data, allowing for further spatial analysis. The coordinate reference system (CRS) was set to EPSG:4326, which is based on latitude and longitude.\nThe resulting GeoDataFrame, Assault21, includes both the original crime data and the newly created geometry column, enabling us to perform spatial analysis and visualization. With this setup, we are able to map the locations of assault incidents, gaining insights into the spatial distribution of these crimes.\nTo visualize the assault incidents in Chicago, we used a combination of a boundary shapefile and the Assault21 GeoDataFrame. First, we retrieved census data for Chicago’s tracts using the c.acs5.state_county_tract function from the census module. Relevant demographic variables, such as total population, were selected and stored in a Pandas DataFrame called chidf. Additionally, a GeoDataFrame representing Chicago’s city boundary was loaded from the GeoJSON file chicagoBoundary.geojson, which defines the city’s limits.\n\nThe map was created using matplotlib and geopandas. The chicagoBoundary GeoDataFrame was plotted first, using a beige color to represent the city’s boundary. The Assault21 GeoDataFrame was overlaid on top, with assault incident locations marked in orange. Markers were kept small to prevent map clutter, providing a clearer visualization of incident distributions.\nFrom the map, we observe that assault incidents are concentrated in the northern and central areas of the city, while the southern region shows fewer incidents. This spatial distribution allows us to identify potential hotspots and explore correlations with other socioeconomic or geographic factors in the following stages of the analysis.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nmpl.rcParams['font.family'] = 'sans-serif'\nmpl.rcParams['font.sans-serif'] = 'Futura'\nmpl.rcParams['font.size'] = 12\n\nfig, ax = plt.subplots(figsize=(10, 10))\n\n# chicagoBoundary.plot(ax=ax, color='beige')\nchicagoBoundary.to_crs(4326).plot(ax=ax, color='black')\n# Assault21.to_crs()\nAssault21.plot(ax=ax, color='#c2e538', markersize=0.1, label='Assault Incidents')\n\nplt.title(\"Assault Incidents in Chicago 2021\")\nplt.xticks([])\nplt.yticks([])\nplt.gca().set_frame_on(False)\n\nplt.gca().set_facecolor('white')\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['left'].set_color('grey')\nplt.gca().spines['bottom'].set_color('grey')\nplt.gca().spines['left'].set_linewidth(0.8)\nplt.gca().spines['bottom'].set_linewidth(0.8)\n\nplt.show()\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hello everyone\n\n\nOn this about page, you might want to add more information about yourself, the project, or course.\n\n\nMy name is Eric Delmelle, the instructor for the course.\nYou can find more information about me on my personal website.\nThis site is an example site showing how to use Quarto for the final project for MUSA 550, during fall 2024.\nWrite something about you\n\nor about something you like\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "DataProcessing/index1.html",
    "href": "DataProcessing/index1.html",
    "title": "Analysis",
    "section": "",
    "text": "Analysis\nThis section includes examples of technical analysis done using Jupyter notebooks. Each sub-section highlights different types of analyses and visualizations. In particular, it highlights that we can easily publish interactive visualizations produced with packages such as hvPlot, altair, or Folium, without losing any of the interactive features.\nOn this page, you might want to share more introductory or background information about the analyses to help guide the reader.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "analysis/2-static-images.html",
    "href": "analysis/2-static-images.html",
    "title": "Showing static visualizations",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and demonstrates how to generate static visualizations with matplotlib, pandas, and seaborn.\nStart by importing the packages we need:\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nLoad the “Palmer penguins” dataset from week 2:\n# Load data on Palmer penguins\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/penguins.csv\")\n# Show the first ten rows\npenguins.head(n=10)    \n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181.0\n3625.0\nfemale\n2007\n\n\n7\nAdelie\nTorgersen\n39.2\n19.6\n195.0\n4675.0\nmale\n2007\n\n\n8\nAdelie\nTorgersen\n34.1\n18.1\n193.0\n3475.0\nNaN\n2007\n\n\n9\nAdelie\nTorgersen\n42.0\n20.2\n190.0\n4250.0\nNaN\n2007"
  },
  {
    "objectID": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "href": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "title": "Showing static visualizations",
    "section": "A simple visualization, 3 different ways",
    "text": "A simple visualization, 3 different ways\n\nI want to scatter flipper length vs. bill length, colored by the penguin species\n\n\nUsing matplotlib\n\n# Setup a dict to hold colors for each species\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Initialize the figure \"fig\" and axes \"ax\"\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Group the data frame by species and loop over each group\n# NOTE: \"group\" will be the dataframe holding the data for \"species\"\nfor species, group_df in penguins.groupby(\"species\"):\n\n    # Plot flipper length vs bill length for this group\n    # Note: we are adding this plot to the existing \"ax\" object\n    ax.scatter(\n        group_df[\"flipper_length_mm\"],\n        group_df[\"bill_length_mm\"],\n        marker=\"o\",\n        label=species,\n        color=color_map[species],\n        alpha=0.75,\n        zorder=10\n    )\n\n# Plotting is done...format the axes!\n\n## Add a legend to the axes\nax.legend(loc=\"best\")\n\n## Add x-axis and y-axis labels\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\n\n## Add the grid of lines\nax.grid(True);\n\n\n\n\n\n\n\n\n\n\nHow about in pandas?\nDataFrames have a built-in “plot” function that can make all of the basic type of matplotlib plots!\nFirst, we need to add a new “color” column specifying the color to use for each species type.\nUse the pd.replace() function: it use a dict to replace values in a DataFrame column.\n\n# Calculate a list of colors\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Map species name to color \npenguins[\"color\"] = penguins[\"species\"].replace(color_map)\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\ncolor\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n#1f77b4\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n#1f77b4\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n#1f77b4\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n#1f77b4\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n#1f77b4\n\n\n\n\n\n\n\nNow plot!\n\n# Same as before: Start by initializing the figure and axes\nfig, myAxes = plt.subplots(figsize=(10, 6))\n\n# Scatter plot two columns, colored by third\n# Use the built-in pandas plot.scatter function\npenguins.plot.scatter(\n    x=\"flipper_length_mm\",\n    y=\"bill_length_mm\",\n    c=\"color\",\n    alpha=0.75,\n    ax=myAxes, # IMPORTANT: Make sure to plot on the axes object we created already!\n    zorder=10\n)\n\n# Format the axes finally\nmyAxes.set_xlabel(\"Flipper Length (mm)\")\nmyAxes.set_ylabel(\"Bill Length (mm)\")\nmyAxes.grid(True);\n\n\n\n\n\n\n\n\nNote: no easy way to get legend added to the plot in this case…\n\n\nSeaborn: statistical data visualization\nSeaborn is designed to plot two columns colored by a third column…\n\n# Initialize the figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# style keywords as dict\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\nstyle = dict(palette=color_map, s=60, edgecolor=\"none\", alpha=0.75, zorder=10)\n\n# use the scatterplot() function\nsns.scatterplot(\n    x=\"flipper_length_mm\",  # the x column\n    y=\"bill_length_mm\",  # the y column\n    hue=\"species\",  # the third dimension (color)\n    data=penguins,  # pass in the data\n    ax=ax,  # plot on the axes object we made\n    **style  # add our style keywords\n)\n\n# Format with matplotlib commands\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\nax.grid(True)\nax.legend(loc=\"best\");"
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Analysis",
    "section": "",
    "text": "Analysis\nThis section includes examples of technical analysis done using Jupyter notebooks. Each sub-section highlights different types of analyses and visualizations. In particular, it highlights that we can easily publish interactive visualizations produced with packages such as hvPlot, altair, or Folium, without losing any of the interactive features.\nOn this page, you might want to share more introductory or background information about the analyses to help guide the reader.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Machine learning (ML) has become a powerful tool for analyzing and predicting crime, enabling law enforcement agencies to enhance their crime prevention strategies through data-driven insights. By leveraging large volumes of historical crime data, ML algorithms can uncover patterns and correlations that may not be immediately evident to human analysts. When combined with geospatial risk models, which account for geographic and temporal factors, these algorithms can forecast the likelihood of crimes occurring in specific areas. This predictive capability allows for more efficient resource allocation, focused patrols in high-risk areas, and a proactive approach to crime prevention.\nGeospatial risk models integrate diverse data sources, such as socioeconomic indicators, weather patterns, and local crime reports, to predict the location and timing of future criminal activity. These models apply spatial analysis techniques to identify crime hotspots, assess neighborhood-level trends, and evaluate risk factors associated with different types of crimes.\nHowever, these approaches have notable limitations. One concern is the potential for bias in historical data, which can reflect discriminatory practices such as racial profiling, leading to the unfair targeting of certain communities. Additionally, focusing law enforcement efforts on areas flagged by these models may create feedback loops, where over-policing inflates crime data in those locations. This selection bias can skew predictions and limit the model’s effectiveness, particularly in identifying latent risks in underreported areas.\nIn this project, we aim to develop a statistical model to predict assault incidents across Chicago neighborhoods. Initially, we considered using a random forest approach, which is effective for handling complex, non-linear relationships. However, given that our goal is to predict counts of crime incidents, we have opted for a Poisson regression model instead. Poisson regression is particularly suitable for modeling count data, such as the number of assault incidents in specific areas, and accounts for the underlying distribution of crime occurrences.\nOur model will consider a range of geospatial and socioeconomic factors to capture the spatial trends of assaults across different neighborhoods. We acknowledge that assault data is influenced by selection bias, as not all incidents are reported, and increased police presence in historically high-crime areas can lead to over-reporting. Additionally, we hypothesize that proximity to certain factors—such as urban decay or community resources like schools or parks—plays a significant role in crime risk.\nBy developing and evaluating a Poisson regression model, we aim to identify the most significant predictors of assault risk. This predictive framework will allow for more efficient resource allocation, focused interventions in high-risk areas, and a proactive approach to crime prevention. Ultimately, this approach supports public safety efforts by reducing crime through targeted strategies and helping law enforcement anticipate and mitigate criminal incidents.\n\n\n\n Back to top"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html",
    "href": "analysis/3-altair-hvplot.html",
    "title": "Altair and Hvplot Charts",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive charts produced using Altair and hvPlot."
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in Altair",
    "text": "Example: Measles Incidence in Altair\nFirst, let’s load the data for measles incidence in wide format:\n\n\nCode\nurl = \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/measles_incidence.csv\"\ndata = pd.read_csv(url, skiprows=2, na_values=\"-\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nWEEK\nALABAMA\nALASKA\nARIZONA\nARKANSAS\nCALIFORNIA\nCOLORADO\nCONNECTICUT\nDELAWARE\n...\nSOUTH DAKOTA\nTENNESSEE\nTEXAS\nUTAH\nVERMONT\nVIRGINIA\nWASHINGTON\nWEST VIRGINIA\nWISCONSIN\nWYOMING\n\n\n\n\n0\n1928\n1\n3.67\nNaN\n1.90\n4.11\n1.38\n8.38\n4.50\n8.58\n...\n5.69\n22.03\n1.18\n0.4\n0.28\nNaN\n14.83\n3.36\n1.54\n0.91\n\n\n1\n1928\n2\n6.25\nNaN\n6.40\n9.91\n1.80\n6.02\n9.00\n7.30\n...\n6.57\n16.96\n0.63\nNaN\n0.56\nNaN\n17.34\n4.19\n0.96\nNaN\n\n\n2\n1928\n3\n7.95\nNaN\n4.50\n11.15\n1.31\n2.86\n8.81\n15.88\n...\n2.04\n24.66\n0.62\n0.2\n1.12\nNaN\n15.67\n4.19\n4.79\n1.36\n\n\n3\n1928\n4\n12.58\nNaN\n1.90\n13.75\n1.87\n13.71\n10.40\n4.29\n...\n2.19\n18.86\n0.37\n0.2\n6.70\nNaN\n12.77\n4.66\n1.64\n3.64\n\n\n4\n1928\n5\n8.03\nNaN\n0.47\n20.79\n2.38\n5.13\n16.80\n5.58\n...\n3.94\n20.05\n1.57\n0.4\n6.70\nNaN\n18.83\n7.37\n2.91\n0.91\n\n\n\n\n5 rows × 53 columns\n\n\n\nThen, use the pandas.melt() function to convert it to tidy format:\n\n\nCode\nannual = data.drop(\"WEEK\", axis=1)\nmeasles = annual.groupby(\"YEAR\").sum().reset_index()\nmeasles = measles.melt(id_vars=\"YEAR\", var_name=\"state\", value_name=\"incidence\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nstate\nincidence\n\n\n\n\n0\n1928\nALABAMA\n334.99\n\n\n1\n1929\nALABAMA\n111.93\n\n\n2\n1930\nALABAMA\n157.00\n\n\n3\n1931\nALABAMA\n337.29\n\n\n4\n1932\nALABAMA\n10.21\n\n\n\n\n\n\n\nFinally, load altair:\n\nimport altair as alt\n\nAnd generate our final data viz:\n\n# use a custom color map\ncolormap = alt.Scale(\n    domain=[0, 100, 200, 300, 1000, 3000],\n    range=[\n        \"#F0F8FF\",\n        \"cornflowerblue\",\n        \"mediumseagreen\",\n        \"#FFEE00\",\n        \"darkorange\",\n        \"firebrick\",\n    ],\n    type=\"sqrt\",\n)\n\n# Vertical line for vaccination year\nthreshold = pd.DataFrame([{\"threshold\": 1963}])\n\n# plot YEAR vs state, colored by incidence\nchart = (\n    alt.Chart(measles)\n    .mark_rect()\n    .encode(\n        x=alt.X(\"YEAR:O\", axis=alt.Axis(title=None, ticks=False)),\n        y=alt.Y(\"state:N\", axis=alt.Axis(title=None, ticks=False)),\n        color=alt.Color(\"incidence:Q\", sort=\"ascending\", scale=colormap, legend=None),\n        tooltip=[\"state\", \"YEAR\", \"incidence\"],\n    )\n    .properties(width=650, height=500)\n)\n\nrule = alt.Chart(threshold).mark_rule(strokeWidth=4).encode(x=\"threshold:O\")\n\nout = chart + rule\nout"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in hvplot",
    "text": "Example: Measles Incidence in hvplot\n\n\n\n\n\n\n\n\n\n\n\nGenerate the same data viz in hvplot:\n\n# Make the heatmap with hvplot\nheatmap = measles.hvplot.heatmap(\n    x=\"YEAR\",\n    y=\"state\",\n    C=\"incidence\", # color each square by the incidence\n    reduce_function=np.sum, # sum the incidence for each state/year\n    frame_height=450,\n    frame_width=600,\n    flip_yaxis=True,\n    rot=90,\n    colorbar=False,\n    cmap=\"viridis\",\n    xlabel=\"\",\n    ylabel=\"\",\n)\n\n# Some additional formatting using holoviews \n# For more info: http://holoviews.org/user_guide/Customizing_Plots.html\nheatmap = heatmap.redim(state=\"State\", YEAR=\"Year\")\nheatmap = heatmap.opts(fontsize={\"xticks\": 0, \"yticks\": 6}, toolbar=\"above\")\nheatmap"
  },
  {
    "objectID": "analysis/1-python-code-blocks.html",
    "href": "analysis/1-python-code-blocks.html",
    "title": "Python code blocks",
    "section": "",
    "text": "This is an example from the Quarto documentation that shows how to mix executable Python code blocks into a markdown file in a “Quarto markdown” .qmd file.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "analysis/4-folium.html",
    "href": "analysis/4-folium.html",
    "title": "Interactive Maps with Folium",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive maps produced using Folium."
  },
  {
    "objectID": "analysis/4-folium.html#finding-the-shortest-route",
    "href": "analysis/4-folium.html#finding-the-shortest-route",
    "title": "Interactive Maps with Folium",
    "section": "Finding the shortest route",
    "text": "Finding the shortest route\nThis example finds the shortest route between the Art Musuem and the Liberty Bell using osmnx.\n\nimport osmnx as ox\n\nFirst, identify the lat/lng coordinates for our places of interest. Use osmnx to download the geometries for the Libery Bell and Art Museum.\n\nphilly_tourism = ox.features_from_place(\"Philadelphia, PA\", tags={\"tourism\": True})\n\n\nart_museum = philly_tourism.query(\"name == 'Philadelphia Museum of Art'\").squeeze()\n\nart_museum.geometry\n\n\n\n\n\n\n\n\n\nliberty_bell = philly_tourism.query(\"name == 'Liberty Bell'\").squeeze()\n\nliberty_bell.geometry\n\n\n\n\n\n\n\n\nNow, extract the lat and lng coordinates\nFor the Art Museum geometry, we can use the .geometry.centroid attribute to calculate the centroid of the building footprint.\n\nliberty_bell_x = liberty_bell.geometry.x\nliberty_bell_y = liberty_bell.geometry.y\n\n\nart_museum_x = art_museum.geometry.centroid.x\nart_museum_y = art_museum.geometry.centroid.y\n\nNext, use osmnx to download the street graph around Center City.\n\nG_cc = ox.graph_from_address(\n    \"City Hall, Philadelphia, USA\", dist=1500, network_type=\"drive\"\n)\n\nNext, identify the nodes in the graph closest to our points of interest.\n\n# Get the origin node (Liberty Bell)\norig_node = ox.nearest_nodes(G_cc, liberty_bell_x, liberty_bell_y)\n\n# Get the destination node (Art Musuem)\ndest_node = ox.nearest_nodes(G_cc, art_museum_x, art_museum_y)\n\nFind the shortest path, based on the distance of the edges:\n\n# Get the shortest path --&gt; just a list of node IDs\nroute = ox.shortest_path(G_cc, orig_node, dest_node, weight=\"length\")\n\nHow about an interactive version?\nosmnx has a helper function ox.utils_graph.route_to_gdf() to convert a route to a GeoDataFrame of edges.\n\nox.utils_graph.route_to_gdf(G_cc, route, weight=\"length\").explore(\n    tiles=\"cartodb positron\",\n    color=\"red\",\n)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/4-folium.html#examining-trash-related-311-requests",
    "href": "analysis/4-folium.html#examining-trash-related-311-requests",
    "title": "Interactive Maps with Folium",
    "section": "Examining Trash-Related 311 Requests",
    "text": "Examining Trash-Related 311 Requests\nFirst, let’s load the dataset from a CSV file and convert to a GeoDataFrame:\n\n\nCode\n# Load the data from a CSV file into a pandas DataFrame\ntrash_requests_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/trash_311_requests_2020.csv\"\n)\n\n# Remove rows with missing geometry\ntrash_requests_df = trash_requests_df.dropna(subset=[\"lat\", \"lon\"])\n\n\n# Create our GeoDataFrame with geometry column created from lon/lat\ntrash_requests = gpd.GeoDataFrame(\n    trash_requests_df,\n    geometry=gpd.points_from_xy(trash_requests_df[\"lon\"], trash_requests_df[\"lat\"]),\n    crs=\"EPSG:4326\",\n)\n\n\nLoad neighborhoods and do the spatial join to associate a neighborhood with each ticket:\n\n\nCode\n# Load the neighborhoods\nneighborhoods = gpd.read_file(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/zillow_neighborhoods.geojson\"\n)\n\n# Do the spatial join to add the \"ZillowName\" column\nrequests_with_hood = gpd.sjoin(\n    trash_requests,\n    neighborhoods.to_crs(trash_requests.crs),\n    predicate=\"within\",\n)\n\n\nLet’s explore the 311 requests in the Greenwich neighborhood of the city:\n\n# Extract out the point tickets for Greenwich\ngreenwich_tickets = requests_with_hood.query(\"ZillowName == 'Greenwich'\")\n\n\n# Get the neighborhood boundary for Greenwich\ngreenwich_geo = neighborhoods.query(\"ZillowName == 'Greenwich'\")\n\ngreenwich_geo.squeeze().geometry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuarto has callout blocks that you can use to emphasize content in different ways. This is a “Note” callout block. More info is available on the Quarto documentation.\n\n\nImport the packages we need:\n\nimport folium\nimport xyzservices\n\nCombine the tickets as markers and the neighborhood boundary on the same Folium map:\n\n# Plot the neighborhood boundary\nm = greenwich_geo.explore(\n    style_kwds={\"weight\": 4, \"color\": \"black\", \"fillColor\": \"none\"},\n    name=\"Neighborhood boundary\",\n    tiles=xyzservices.providers.CartoDB.Voyager,\n)\n\n\n# Add the individual tickets as circle markers and style them\ngreenwich_tickets.explore(\n    m=m,  # Add to the existing map!\n    marker_kwds={\"radius\": 7, \"fill\": True, \"color\": \"crimson\"},\n    marker_type=\"circle_marker\", # or 'marker' or 'circle'\n    name=\"Tickets\",\n)\n\n# Hse folium to add layer control\nfolium.LayerControl().add_to(m)\n\nm  # show map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Predicting Assault Incidents in Chicago: A Poisson Regression Model Integrating Geospatial and Socioeconomic Variables",
    "section": "",
    "text": "Machine learning (ML) is an invaluable tool for crime analysis and prediction, providing law enforcement with data-driven insights to enhance crime prevention. This project aims to forecast assault incidents across Chicago neighborhoods using Poisson regression, which is ideal for modeling count data like crime occurrences.\nWe utilize a variety of geospatial and socioeconomic factors to uncover spatial trends and key assault risk predictors. By integrating geospatial risk models with socioeconomic indicators and local crime reports, our model identifies areas with high crime probabilities. The Poisson regression model, fitted with 1,098 grid cells, shows a log-likelihood of -9330.8, signifying a good fit. Deviance and Pearson chi-square values (14,511 and 19,500 respectively) highlight the model’s explanatory power, with a pseudo R-squared value of 0.9994 indicating robust data variability capture.\nAll predictors are statistically significant (p &lt; 0.001). Notably, non-functional streetlights (coefficient: 0.1229) and ShotSpotter-detected incidents (coefficient: 0.0333) have strong positive associations with assault counts. Liquor retail stores (coefficient: 0.0015) and graffiti incidents (coefficient: 0.0010) are also significant contributors.\nThese findings underscore the importance of urban infrastructure and crime detection technology in understanding and preventing assaults. Our predictive framework facilitates efficient resource allocation and targeted interventions in high-risk areas, aiding law enforcement in proactively mitigating criminal incidents. Through data-driven strategies, we aim to enhance community safety and improve crime prevention efficacy.\n\nThis study was conducted in completion of a Final Project for MUSA 550"
  },
  {
    "objectID": "index.html#study-abstract",
    "href": "index.html#study-abstract",
    "title": "Predicting Assault Incidents in Chicago: A Poisson Regression Model Integrating Geospatial and Socioeconomic Variables",
    "section": "",
    "text": "Machine learning (ML) is an invaluable tool for crime analysis and prediction, providing law enforcement with data-driven insights to enhance crime prevention. This project aims to forecast assault incidents across Chicago neighborhoods using Poisson regression, which is ideal for modeling count data like crime occurrences.\nWe utilize a variety of geospatial and socioeconomic factors to uncover spatial trends and key assault risk predictors. By integrating geospatial risk models with socioeconomic indicators and local crime reports, our model identifies areas with high crime probabilities. The Poisson regression model, fitted with 1,098 grid cells, shows a log-likelihood of -9330.8, signifying a good fit. Deviance and Pearson chi-square values (14,511 and 19,500 respectively) highlight the model’s explanatory power, with a pseudo R-squared value of 0.9994 indicating robust data variability capture.\nAll predictors are statistically significant (p &lt; 0.001). Notably, non-functional streetlights (coefficient: 0.1229) and ShotSpotter-detected incidents (coefficient: 0.0333) have strong positive associations with assault counts. Liquor retail stores (coefficient: 0.0015) and graffiti incidents (coefficient: 0.0010) are also significant contributors.\nThese findings underscore the importance of urban infrastructure and crime detection technology in understanding and preventing assaults. Our predictive framework facilitates efficient resource allocation and targeted interventions in high-risk areas, aiding law enforcement in proactively mitigating criminal incidents. Through data-driven strategies, we aim to enhance community safety and improve crime prevention efficacy.\n\nThis study was conducted in completion of a Final Project for MUSA 550"
  },
  {
    "objectID": "python_final_code/Final-Copy1.html",
    "href": "python_final_code/Final-Copy1.html",
    "title": "MUSA 550",
    "section": "",
    "text": "!pip install census\n!pip install us\n!pip install sodapy\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport geopandas as gpd\nfrom census import Census\nfrom us import states\nimport os\nfrom sodapy import Socrata\nfrom shapely.geometry import Point\nimport numpy as np\nfrom scipy.stats import gaussian_kde\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom sklearn.neighbors import KernelDensity\nfrom shapely.geometry import box\nfrom matplotlib.colors import Normalize\nfrom matplotlib.colorbar import ColorbarBase\nimport seaborn as sns\n\nRequirement already satisfied: census in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (0.8.23)\nRequirement already satisfied: requests&gt;=1.1.0 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from census) (2.31.0)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from requests&gt;=1.1.0-&gt;census) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from requests&gt;=1.1.0-&gt;census) (3.8)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from requests&gt;=1.1.0-&gt;census) (1.26.19)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from requests&gt;=1.1.0-&gt;census) (2024.8.30)\nRequirement already satisfied: us in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (3.1.1)\nRequirement already satisfied: jellyfish==0.11.2 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from us) (0.11.2)\nRequirement already satisfied: sodapy in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (2.2.0)\nRequirement already satisfied: requests&gt;=2.28.1 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from sodapy) (2.31.0)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from requests&gt;=2.28.1-&gt;sodapy) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from requests&gt;=2.28.1-&gt;sodapy) (3.8)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from requests&gt;=2.28.1-&gt;sodapy) (1.26.19)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from requests&gt;=2.28.1-&gt;sodapy) (2024.8.30)\n\n\n\n# Unauthenticated client only works with public data sets. Note 'None'\n# in place of application token, and no username or password:\n# client = Socrata(\"data.cityofchicago.org\", None)\n\n# Example authenticated client (needed for non-public datasets):\nclient = Socrata(\"data.cityofchicago.org\",\n                  \"PXGs3LAGSv2IZaGVJVPf1M0Fz\",\n                  username=\"jijinc@upenn.edu\",\n                  password=\"8m9reD@XfA$Z5W.\")\n\n# First 2000 results, returned as JSON from API / converted to Python list of\n# dictionaries by sodapy.\nresults = client.get(\"dwme-t96c\", limit=210000)\n\n# Convert to pandas DataFrame\nresults_df = pd.DataFrame.from_records(results)\n\n\nresults_df\n\n\n\n\n\n\n\n\ndate\nlocation\ndistrict\nblock\ny_coordinate\nlatitude\ndescription\nlocation_description\nupdated_on\ncommunity_area\n...\nward\ncase_number\nyear\ndomestic\nfbi_code\nlongitude\nbeat\nprimary_type\narrest\nid\n\n\n\n\n0\n2021-12-31T23:59:00.000\n{'latitude': '41.894327846', 'human_address': ...\n018\n006XX N STATE ST\n1904872\n41.894327846\nLIQUOR LICENSE VIOLATION\nHOTEL / MOTEL\n2022-01-08T15:39:35.000\n8\n...\n42\nJF103542\n2021\nFalse\n22\n-87.62814321\n1832\nLIQUOR LAW VIOLATION\nTrue\n12584710\n\n\n1\n2021-12-31T23:59:00.000\nNaN\n008\n023XX W 64TH ST\nNaN\nNaN\nOVER $500\nSCHOOL - PUBLIC BUILDING\n2022-02-05T15:40:37.000\n66\n...\n16\nJF133671\n2021\nFalse\n06\nNaN\n0825\nTHEFT\nFalse\n12609426\n\n\n2\n2021-12-31T23:58:00.000\n{'latitude': '41.745680051', 'human_address': ...\n004\n082XX S MARQUETTE AVE\n1850868\n41.745680051\nHOME INVASION\nAPARTMENT\n2022-01-07T15:44:31.000\n46\n...\n7\nJF100042\n2021\nTrue\n05\n-87.558851198\n0423\nBURGLARY\nFalse\n12581979\n\n\n3\n2021-12-31T23:55:00.000\n{'latitude': '41.734916889', 'human_address': ...\n022\n087XX S WOOD ST\n1846697\n41.734916889\nTO PROPERTY\nRESIDENCE\n2022-01-07T15:44:31.000\n71\n...\n21\nJF102969\n2021\nFalse\n14\n-87.66790905\n2221\nCRIMINAL DAMAGE\nFalse\n12584234\n\n\n4\n2021-12-31T23:50:00.000\n{'latitude': '41.770713582', 'human_address': ...\n003\n068XX S CHAMPLAIN AVE\n1859870\n41.770713582\nTO RESIDENCE\nAPARTMENT\n2022-01-07T15:44:31.000\n42\n...\n20\nJF100045\n2021\nFalse\n26\n-87.609358643\n0321\nCRIMINAL TRESPASS\nFalse\n12581975\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n209397\n2021-01-01T00:00:00.000\n{'latitude': '41.725762292', 'human_address': ...\n004\n093XX S BENNETT AVE\n1843564\n41.725762292\nAGGRAVATED SEXUAL ASSAULT OF CHILD BY FAMILY M...\nRESIDENCE\n2021-04-14T15:41:04.000\n48\n...\n8\nJE197150\n2021\nTrue\n02\n-87.57787575\n0413\nOFFENSE INVOLVING CHILDREN\nFalse\n12338506\n\n\n209398\n2021-01-01T00:00:00.000\n{'latitude': '41.958971937', 'human_address': ...\n019\n042XX N BROADWAY\n1928373\n41.958971937\nFINANCIAL IDENTITY THEFT OVER $ 300\nAPARTMENT\n2021-11-13T15:39:51.000\n3\n...\n46\nJE441404\n2021\nFalse\n11\n-87.653365753\n1915\nDECEPTIVE PRACTICE\nFalse\n12537851\n\n\n209399\n2021-01-01T00:00:00.000\n{'latitude': '42.019380398', 'human_address': ...\n024\n016XX W HOWARD ST\n1950346\n42.019380398\nFINANCIAL IDENTITY THEFT $300 AND UNDER\nAPARTMENT\n2021-05-04T15:40:05.000\n1\n...\n49\nJE216275\n2021\nTrue\n11\n-87.672249127\n2422\nDECEPTIVE PRACTICE\nFalse\n12354069\n\n\n209400\n2021-01-01T00:00:00.000\n{'latitude': '41.778229684', 'human_address': ...\n008\n063XX S ROCKWELL ST\n1862437\n41.778229684\nAGGRAVATED CRIMINAL SEXUAL ABUSE BY FAMILY MEMBER\nRESIDENCE\n2023-02-21T15:43:50.000\n66\n...\n16\nJE447436\n2021\nTrue\n17\n-87.688505893\n0825\nOFFENSE INVOLVING CHILDREN\nFalse\n12545674\n\n\n209401\n2021-01-01T00:00:00.000\n{'latitude': '41.740561073', 'human_address': ...\n006\n084XX S MORGAN ST\n1848795\n41.740561073\nPOSSESS - SYNTHETIC DRUGS\nSTREET\n2021-01-16T15:39:23.000\n71\n...\n21\nJE100603\n2021\nFalse\n18\n-87.648610446\n0613\nNARCOTICS\nTrue\n12258978\n\n\n\n\n209402 rows × 22 columns\n\n\n\n\nassault = results_df[results_df[\"primary_type\"] == \"ASSAULT\"].copy()\n\nassault['geometry'] = assault.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)\n\nAssault21 = gpd.GeoDataFrame(assault, geometry='geometry')\n\nAssault21.crs = \"EPSG:4326\"\n\nAssault21\n\n\n\n\n\n\n\n\ndate\nlocation\ndistrict\nblock\ny_coordinate\nlatitude\ndescription\nlocation_description\nupdated_on\ncommunity_area\n...\ncase_number\nyear\ndomestic\nfbi_code\nlongitude\nbeat\nprimary_type\narrest\nid\ngeometry\n\n\n\n\n5\n2021-12-31T23:50:00.000\n{'latitude': '41.800089835', 'human_address': ...\n008\n051XX S SPAULDING AVE\n1870367\n41.800089835\nSIMPLE\nAPARTMENT\n2022-01-07T15:44:31.000\n63\n...\nJF100292\n2021\nFalse\n08A\n-87.706195287\n0822\nASSAULT\nFalse\n12582027\nPOINT (-87.70620 41.80009)\n\n\n7\n2021-12-31T23:46:00.000\n{'latitude': '41.745827204', 'human_address': ...\n006\n009XX E 82ND ST\n1850818\n41.745827204\nAGGRAVATED - HANDGUN\nAPARTMENT\n2022-01-07T15:44:31.000\n44\n...\nJF100025\n2021\nTrue\n04A\n-87.602254489\n0631\nASSAULT\nFalse\n12581871\nPOINT (-87.60225 41.74583)\n\n\n23\n2021-12-31T23:06:00.000\n{'latitude': '41.89668614', 'human_address': '...\n018\n0000X E CHICAGO AVE\n1905733\n41.89668614\nSIMPLE\nRESTAURANT\n2022-01-07T15:44:31.000\n8\n...\nJE494453\n2021\nFalse\n08A\n-87.627412043\n1833\nASSAULT\nTrue\n12581778\nPOINT (-87.62741 41.89669)\n\n\n58\n2021-12-31T22:05:00.000\n{'latitude': '41.785957604', 'human_address': ...\n008\n039XX W 59TH ST\n1865187\n41.785957604\nAGGRAVATED - HANDGUN\nAPARTMENT\n2022-01-07T15:44:31.000\n65\n...\nJE494409\n2021\nTrue\n04A\n-87.721623241\n0822\nASSAULT\nFalse\n12582370\nPOINT (-87.72162 41.78596)\n\n\n65\n2021-12-31T22:00:00.000\n{'latitude': '41.801535279', 'human_address': ...\n009\n009XX W 51ST ST\n1871013\n41.801535279\nSIMPLE\nAPARTMENT\n2022-01-07T15:44:31.000\n61\n...\nJF100618\n2021\nTrue\n08A\n-87.64885291\n0934\nASSAULT\nFalse\n12582361\nPOINT (-87.64885 41.80154)\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n209118\n2021-01-01T01:32:00.000\n{'latitude': '41.868043561', 'human_address': ...\n011\n038XX W FILLMORE ST\n1895100\n41.868043561\nSIMPLE\nAPARTMENT\n2021-11-17T15:40:16.000\n29\n...\nJE100548\n2021\nTrue\n08A\n-87.720966147\n1133\nASSAULT\nFalse\n12258986\nPOINT (-87.72097 41.86804)\n\n\n209129\n2021-01-01T01:22:00.000\n{'latitude': '41.781326622', 'human_address': ...\n007\n009XX W 63RD PKWY\n1863652\n41.781326622\nSIMPLE\nAPARTMENT\n2021-01-16T15:39:23.000\n68\n...\nJE100098\n2021\nFalse\n08A\n-87.647516964\n0712\nASSAULT\nFalse\n12258648\nPOINT (-87.64752 41.78133)\n\n\n209138\n2021-01-01T01:00:00.000\n{'latitude': '41.77999793', 'human_address': '...\n003\n001XX W 63RD ST\n1863208\n41.77999793\nSIMPLE\nGAS STATION\n2021-01-16T15:39:23.000\n68\n...\nJE100026\n2021\nFalse\n08A\n-87.629294594\n0311\nASSAULT\nFalse\n12259387\nPOINT (-87.62929 41.78000)\n\n\n209323\n2021-01-01T00:00:00.000\n{'latitude': '41.910909931', 'human_address': ...\n025\n016XX N CICERO AVE\n1910673\n41.910909931\nAGGRAVATED - HANDGUN\nALLEY\n2021-01-16T15:39:23.000\n25\n...\nJE100369\n2021\nFalse\n04A\n-87.745898897\n2533\nASSAULT\nFalse\n12258861\nPOINT (-87.74590 41.91091)\n\n\n209388\n2021-01-01T00:00:00.000\n{'latitude': '41.846229464', 'human_address': ...\n010\n025XX S CALIFORNIA AVE\n1887202\n41.846229464\nSIMPLE\nAPARTMENT\n2021-03-19T15:39:40.000\n30\n...\nJE171603\n2021\nTrue\n08A\n-87.695259244\n1034\nASSAULT\nFalse\n12317576\nPOINT (-87.69526 41.84623)\n\n\n\n\n20343 rows × 23 columns\n\n\n\n\nc = Census(\"0e028dfcee38f844e89c39d01870f6a964f675a2\")\n\n\nchicensus = c.acs5.state_county_tract(fields = ('NAME', 'C17002_001E', 'C17002_002E', 'C17002_003E', 'B01003_001E'),\n                                      state_fips = states.IL.fips,\n                                      county_fips = \"031\",\n                                      tract = \"*\",\n                                      year = 2021)\n\nchidf = pd.DataFrame(chicensus)\n\n\nchidf\n\n\n\n\n\n\n\n\nNAME\nC17002_001E\nC17002_002E\nC17002_003E\nB01003_001E\nstate\ncounty\ntract\n\n\n\n\n0\nCensus Tract 101, Cook County, Illinois\n4439.0\n495.0\n692.0\n4534.0\n17\n031\n010100\n\n\n1\nCensus Tract 102.01, Cook County, Illinois\n8219.0\n1628.0\n1271.0\n8232.0\n17\n031\n010201\n\n\n2\nCensus Tract 102.02, Cook County, Illinois\n2813.0\n116.0\n396.0\n3124.0\n17\n031\n010202\n\n\n3\nCensus Tract 103, Cook County, Illinois\n5189.0\n251.0\n362.0\n6085.0\n17\n031\n010300\n\n\n4\nCensus Tract 104, Cook County, Illinois\n3011.0\n385.0\n330.0\n4587.0\n17\n031\n010400\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1327\nCensus Tract 8446, Cook County, Illinois\n2307.0\n151.0\n191.0\n2490.0\n17\n031\n844600\n\n\n1328\nCensus Tract 8447, Cook County, Illinois\n1573.0\n173.0\n202.0\n1573.0\n17\n031\n844700\n\n\n1329\nCensus Tract 9800, Cook County, Illinois\n0.0\n0.0\n0.0\n0.0\n17\n031\n980000\n\n\n1330\nCensus Tract 9801, Cook County, Illinois\n0.0\n0.0\n0.0\n0.0\n17\n031\n980100\n\n\n1331\nCensus Tract 9900, Cook County, Illinois\n0.0\n0.0\n0.0\n0.0\n17\n031\n990000\n\n\n\n\n1332 rows × 8 columns\n\n\n\n\nchicagoBoundary = gpd.read_file(\"data/chicagoBoundary.geojson\").to_crs(3435)\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nmpl.rcParams['font.family'] = 'sans-serif'\nmpl.rcParams['font.sans-serif'] = 'Futura'\nmpl.rcParams['font.size'] = 12\n\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# chicagoBoundary.plot(ax=ax, color='beige')\nchicagoBoundary.to_crs(4326).plot(ax=ax, color='black')\n# Assault21.to_crs()\nAssault21.plot(ax=ax, color='#c2e538', markersize=0.1, label='Assault Incidents')\n\nplt.title(\"Assault Incidents in Chicago 2021\")\nplt.xticks([])\nplt.yticks([])\nplt.gca().set_frame_on(False)\n\nplt.gca().set_facecolor('white')\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['left'].set_color('grey')\nplt.gca().spines['bottom'].set_color('grey')\nplt.gca().spines['left'].set_linewidth(0.8)\nplt.gca().spines['bottom'].set_linewidth(0.8)\n\nplt.show()\n\n\n\n\n\n\n\n\n\ncoords = np.array(list(zip(Assault21.geometry.x, Assault21.geometry.y)))\ncoords = coords[~np.isnan(coords).any(axis=1)]\n\n\ndef create_fishnet(boundary, cellsize):\n    bounds = boundary.total_bounds  # Get bounding box of the area\n    xmin, ymin, xmax, ymax = bounds\n    cols = np.arange(xmin, xmax + cellsize, cellsize)\n    rows = np.arange(ymin, ymax + cellsize, cellsize)\n    polygons = [\n        box(x, y, x + cellsize, y + cellsize)\n        for x in cols[:-1] for y in rows[:-1]\n    ]\n    fishnet = gpd.GeoDataFrame(geometry=polygons, crs=boundary.crs)\n    # Intersect with boundary\n    fishnet = fishnet[fishnet.intersects(boundary.unary_union)]\n    fishnet[\"uniqueID\"] = range(1, len(fishnet) + 1)\n    return fishnet\n\n\n# Generate fishnet\nfishnet = create_fishnet(chicagoBoundary, cellsize=2500)\n\n# Plot fishnet\nfig, ax = plt.subplots(figsize=(10, 10))\nfishnet.plot(ax=ax, edgecolor=\"white\", facecolor=\"#c2e538\")\nchicagoBoundary.boundary.plot(ax=ax, color=\"black\", linewidth=0.8)\nax.set_title(\"Fishnet of Chicago\", fontsize=12, fontweight=\"bold\")\nax.axis(\"off\")\nplt.show()\n\n/Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages/shapely/predicates.py:798: RuntimeWarning: invalid value encountered in intersects\n  return lib.intersects(a, b, **kwargs)\n\n\n\n\n\n\n\n\n\n\n# Count of Assaults in each fishnet grid\nAssault21[\"countAssault\"] = 1\nAssault21_net = gpd.sjoin(fishnet, Assault21.to_crs(3435), how=\"left\", op=\"contains\")\nAssault21_net = Assault21_net.groupby(\"uniqueID\").agg(\n    countAssault=(\"countAssault\", \"sum\"),\n    geometry=(\"geometry\", \"first\")\n).reset_index()\n\nAssault21\n\n/Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n  if await self.run_code(code, result, async_=asy):\n\n\n\n\n\n\n\n\n\ndate\nlocation\ndistrict\nblock\ny_coordinate\nlatitude\ndescription\nlocation_description\nupdated_on\ncommunity_area\n...\nyear\ndomestic\nfbi_code\nlongitude\nbeat\nprimary_type\narrest\nid\ngeometry\ncountAssault\n\n\n\n\n5\n2021-12-31T23:50:00.000\n{'latitude': '41.800089835', 'human_address': ...\n008\n051XX S SPAULDING AVE\n1870367\n41.800089835\nSIMPLE\nAPARTMENT\n2022-01-07T15:44:31.000\n63\n...\n2021\nFalse\n08A\n-87.706195287\n0822\nASSAULT\nFalse\n12582027\nPOINT (-87.70620 41.80009)\n1\n\n\n7\n2021-12-31T23:46:00.000\n{'latitude': '41.745827204', 'human_address': ...\n006\n009XX E 82ND ST\n1850818\n41.745827204\nAGGRAVATED - HANDGUN\nAPARTMENT\n2022-01-07T15:44:31.000\n44\n...\n2021\nTrue\n04A\n-87.602254489\n0631\nASSAULT\nFalse\n12581871\nPOINT (-87.60225 41.74583)\n1\n\n\n23\n2021-12-31T23:06:00.000\n{'latitude': '41.89668614', 'human_address': '...\n018\n0000X E CHICAGO AVE\n1905733\n41.89668614\nSIMPLE\nRESTAURANT\n2022-01-07T15:44:31.000\n8\n...\n2021\nFalse\n08A\n-87.627412043\n1833\nASSAULT\nTrue\n12581778\nPOINT (-87.62741 41.89669)\n1\n\n\n58\n2021-12-31T22:05:00.000\n{'latitude': '41.785957604', 'human_address': ...\n008\n039XX W 59TH ST\n1865187\n41.785957604\nAGGRAVATED - HANDGUN\nAPARTMENT\n2022-01-07T15:44:31.000\n65\n...\n2021\nTrue\n04A\n-87.721623241\n0822\nASSAULT\nFalse\n12582370\nPOINT (-87.72162 41.78596)\n1\n\n\n65\n2021-12-31T22:00:00.000\n{'latitude': '41.801535279', 'human_address': ...\n009\n009XX W 51ST ST\n1871013\n41.801535279\nSIMPLE\nAPARTMENT\n2022-01-07T15:44:31.000\n61\n...\n2021\nTrue\n08A\n-87.64885291\n0934\nASSAULT\nFalse\n12582361\nPOINT (-87.64885 41.80154)\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n209118\n2021-01-01T01:32:00.000\n{'latitude': '41.868043561', 'human_address': ...\n011\n038XX W FILLMORE ST\n1895100\n41.868043561\nSIMPLE\nAPARTMENT\n2021-11-17T15:40:16.000\n29\n...\n2021\nTrue\n08A\n-87.720966147\n1133\nASSAULT\nFalse\n12258986\nPOINT (-87.72097 41.86804)\n1\n\n\n209129\n2021-01-01T01:22:00.000\n{'latitude': '41.781326622', 'human_address': ...\n007\n009XX W 63RD PKWY\n1863652\n41.781326622\nSIMPLE\nAPARTMENT\n2021-01-16T15:39:23.000\n68\n...\n2021\nFalse\n08A\n-87.647516964\n0712\nASSAULT\nFalse\n12258648\nPOINT (-87.64752 41.78133)\n1\n\n\n209138\n2021-01-01T01:00:00.000\n{'latitude': '41.77999793', 'human_address': '...\n003\n001XX W 63RD ST\n1863208\n41.77999793\nSIMPLE\nGAS STATION\n2021-01-16T15:39:23.000\n68\n...\n2021\nFalse\n08A\n-87.629294594\n0311\nASSAULT\nFalse\n12259387\nPOINT (-87.62929 41.78000)\n1\n\n\n209323\n2021-01-01T00:00:00.000\n{'latitude': '41.910909931', 'human_address': ...\n025\n016XX N CICERO AVE\n1910673\n41.910909931\nAGGRAVATED - HANDGUN\nALLEY\n2021-01-16T15:39:23.000\n25\n...\n2021\nFalse\n04A\n-87.745898897\n2533\nASSAULT\nFalse\n12258861\nPOINT (-87.74590 41.91091)\n1\n\n\n209388\n2021-01-01T00:00:00.000\n{'latitude': '41.846229464', 'human_address': ...\n010\n025XX S CALIFORNIA AVE\n1887202\n41.846229464\nSIMPLE\nAPARTMENT\n2021-03-19T15:39:40.000\n30\n...\n2021\nTrue\n08A\n-87.695259244\n1034\nASSAULT\nFalse\n12317576\nPOINT (-87.69526 41.84623)\n1\n\n\n\n\n20343 rows × 24 columns\n\n\n\n\nAssault21.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\nAssault21_net[\"countAssault\"] = Assault21_net[\"countAssault\"].fillna(0)\n\n# Add uniqueID and cross-validation IDs\nAssault21_net[\"uniqueID\"] = range(1, len(Assault21_net) + 1)\nAssault21_net[\"cvID\"] = np.random.randint(\n    low=1, high=(len(Assault21_net) // 24) + 1, size=len(Assault21_net)\n)\n\nAssault21_net\n\n\n\n\n\n\n\n\nuniqueID\ncountAssault\ngeometry\ncvID\n\n\n\n\n0\n1\n1.0\nPOLYGON ((1121801.540 1918892.156, 1121801.540...\n44\n\n\n1\n2\n8.0\nPOLYGON ((1121801.540 1921392.156, 1121801.540...\n16\n\n\n2\n3\n7.0\nPOLYGON ((1121801.540 1923892.156, 1121801.540...\n35\n\n\n3\n4\n0.0\nPOLYGON ((1121801.540 1926392.156, 1121801.540...\n36\n\n\n4\n5\n0.0\nPOLYGON ((1121801.540 1931392.156, 1121801.540...\n28\n\n\n...\n...\n...\n...\n...\n\n\n1093\n1094\n0.0\nPOLYGON ((1206801.540 1836392.156, 1206801.540...\n10\n\n\n1094\n1095\n0.0\nPOLYGON ((1206801.540 1838892.156, 1206801.540...\n10\n\n\n1095\n1096\n0.0\nPOLYGON ((1206801.540 1841392.156, 1206801.540...\n5\n\n\n1096\n1097\n0.0\nPOLYGON ((1206801.540 1843892.156, 1206801.540...\n20\n\n\n1097\n1098\n0.0\nPOLYGON ((1206801.540 1848892.156, 1206801.540...\n17\n\n\n\n\n1098 rows × 4 columns\n\n\n\n\n# Plot Assault Counts\nfig, ax = plt.subplots()\nAssault21_net = gpd.GeoDataFrame(Assault21_net, geometry='geometry')  # Ensure 'geometry' column exists\nAssault21_net.plot(column='countAssault', ax=ax, legend=True, cmap='viridis', legend_kwds={'label': \"Assault Counts\"})\n\nplt.title(\"Count of Assaults for the Fishnet\", fontsize=12, fontweight='bold')\nplt.axis('off')\nplt.gca().set_xticklabels([])\nplt.gca().set_yticklabels([])\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['left'].set_visible(False)\nplt.gca().spines['bottom'].set_visible(False)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nif Assault21_net['countAssault'].isna().any():\n    Assault21_net = Assault21_net.dropna()\n\nplt.figure(figsize=(10, 6))\nsns.histplot(Assault21_net['countAssault'], bins=30, color=\"lightblue\", edgecolor=\"#e9ecef\")\nplt.title(\"Distribution of Assaults in Chicago (2021)\", fontsize=14, fontweight='bold')\nplt.xlabel(\"Assault Incidents\", fontsize=10)\nplt.ylabel(\"Count\", fontsize=10)\nplt.xticks(rotation=45, ha='right', fontsize=8)\nplt.yticks(fontsize=8)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.gca().set_facecolor('white')\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['left'].set_color('grey')\nplt.gca().spines['bottom'].set_color('grey')\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.kdeplot(data=Assault21_net, x='countAssault', fill=True, color='lightblue', alpha=0.5)\nplt.title(\"Density Plot of Assaults in Chicago (2021)\")\nplt.xlabel(\"Assault Incidents\")\nplt.ylabel(\"Density\")\nplt.show()\n\n\n\n\n\n\n\n\n\ngraffiti = client.get(\"hec5-y4x5\", limit=1000000)\n\ngraffiti = pd.DataFrame.from_records(graffiti)\n\n\ngraffiti['year'] = graffiti['creation_date'].str[:4]\ngraffiti = graffiti[graffiti['year'] == \"2018\"]\ngraffiti = graffiti[graffiti['where_is_the_graffiti_located_'].isin([\"Front\", \"Rear\", \"Side\"])]\ngraffiti = graffiti[['latitude', 'longitude']].dropna()\n\ngraffiti = gpd.GeoDataFrame(\n    graffiti, \n    geometry=gpd.points_from_xy(graffiti.longitude, graffiti.latitude),\n    crs=\"EPSG:4326\"\n)\n\ngraffiti = graffiti.to_crs(fishnet.crs)\ngraffiti['Legend'] = \"Graffiti\"\n\n\nstreetLightsOut = client.get(\"zuxi-7xem\", limit=1000000)\n\nstreetLightsOut = pd.DataFrame.from_records(streetLightsOut)\n\n\nstreetLightsOut['year'] = streetLightsOut['creation_date'].str[:4]\nstreetLightsOut = streetLightsOut[streetLightsOut['year'] == \"2018\"]\nstreetLightsOut = streetLightsOut[['latitude', 'longitude']].dropna()\nstreetLightsOut = gpd.GeoDataFrame(streetLightsOut, geometry=gpd.points_from_xy(streetLightsOut.longitude, streetLightsOut.latitude), crs=\"EPSG:4326\")\nstreetLightsOut = streetLightsOut.to_crs(fishnet.crs)\nstreetLightsOut['Legend'] = \"streetLightsOut\"\n\n\nliquorRetail = client.get(\"nrmj-3kcf\", limit=1000000)\n\nliquorRetail = pd.DataFrame.from_records(liquorRetail)\n\n\nliquorRetail = liquorRetail[liquorRetail['business_activity'] == \"Retail Sales of Packaged Liquor\"]\nliquorRetail = liquorRetail[['latitude', 'longitude']].dropna()\nliquorRetail = gpd.GeoDataFrame(liquorRetail, geometry=gpd.points_from_xy(liquorRetail.longitude, liquorRetail.latitude), crs=\"EPSG:4326\")\nliquorRetail = liquorRetail.to_crs(fishnet.crs)\nliquorRetail['Legend'] = \"liquorRetail\"\n\n\nshotSpotter = client.get(\"3h7q-7mdb\", limit=1000000)\n\nshotSpotter = pd.DataFrame.from_records(shotSpotter)\n\n\nshotSpotter = shotSpotter[['latitude', 'longitude']].dropna()\nshotSpotter = gpd.GeoDataFrame(shotSpotter, geometry=gpd.points_from_xy(shotSpotter.longitude, shotSpotter.latitude), crs=\"EPSG:4326\")\nshotSpotter = shotSpotter.to_crs(fishnet.crs)\nshotSpotter['Legend'] = \"shotSpotter\"\n\n\nvariable_net = pd.concat([streetLightsOut, liquorRetail, graffiti, shotSpotter]) \\\n                .sjoin(fishnet, how=\"inner\", predicate='within') #\\\n\n\nvariable_net = gpd.GeoDataFrame(variable_net, geometry='geometry', crs=\"EPSG:4326\")\n\n\nvariable_net\n\n\n\n\n\n\n\n\nlatitude\nlongitude\ngeometry\nLegend\nindex_right\nuniqueID\n\n\n\n\n0\n41.66329091907985\n-87.63781345972885\nPOINT (1174294.60116 1820662.65208)\nstreetLightsOut\n1178\n714\n\n\n78\n41.66252879010104\n-87.63779900008863\nPOINT (1174300.79464 1820384.97376)\nstreetLightsOut\n1178\n714\n\n\n1956\n41.66324371090597\n-87.64150879096508\nPOINT (1173285.00739 1820637.32755)\nstreetLightsOut\n1178\n714\n\n\n2003\n41.66443411231659\n-87.64146080782693\nPOINT (1173294.63443 1821071.19997)\nstreetLightsOut\n1178\n714\n\n\n2033\n41.663283479526456\n-87.63900742937679\nPOINT (1173968.37640 1820657.31208)\nstreetLightsOut\n1178\n714\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n164967\n41.797358504\n-87.801782759\nPOINT (1129189.20309 1869196.50642)\nshotSpotter\n190\n31\n\n\n203451\n41.796964146\n-87.802170246\nPOINT (1129084.43377 1869052.15201)\nshotSpotter\n190\n31\n\n\n175163\n41.658698869\n-87.537112847\nPOINT (1201826.08226 1819227.36766)\nshotSpotter\n1850\n1071\n\n\n187824\n41.825602504\n-87.737276159\nPOINT (1146707.10167 1879603.84141)\nshotSpotter\n586\n209\n\n\n203353\n41.781612269\n-87.572185247\nPOINT (1191845.70343 1863929.45790)\nshotSpotter\n1644\n1014\n\n\n\n\n295042 rows × 6 columns\n\n\n\n\nlegend_values = variable_net['Legend'].unique()\n\n# Create subplots\nfig, axes = plt.subplots(1, len(legend_values), figsize=(20, 5), sharex=True, sharey=True)\n\nfor ax, legend_value in zip(axes, legend_values):\n    subset = variable_net[variable_net['Legend'] == legend_value]\n    subset.plot(ax=ax, marker='o', color='blue', markersize=10, alpha=0.5, aspect=1)\n    ax.set_title(f\"Legend: {legend_value}\")\n    ax.set_aspect('equal')  # Set aspect to 'equal' manually\n    ax.axis('on')  # Keep axis visible for better debugging\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n!pip install esda\n\nfrom libpysal.weights import Queen\nfrom esda.moran import Moran_Local\n\nCollecting esda\n  Obtaining dependency information for esda from https://files.pythonhosted.org/packages/a0/1b/84eaa84fa0e2b56464665f1d2135e0afe8ab1df481e6aa1fcdcb480032d6/esda-2.6.0-py3-none-any.whl.metadata\n  Downloading esda-2.6.0-py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: geopandas&gt;=0.12 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from esda) (0.13.2)\nRequirement already satisfied: libpysal&gt;=4.12 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from esda) (4.12.1)\nRequirement already satisfied: numpy&gt;=1.24 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from esda) (1.24.4)\nRequirement already satisfied: pandas&gt;1.5 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from esda) (1.5.3)\nRequirement already satisfied: scikit-learn&gt;=1.2 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from esda) (1.3.0)\nRequirement already satisfied: scipy&gt;=1.9 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from esda) (1.14.1)\nRequirement already satisfied: shapely&gt;=2.0 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from esda) (2.0.1)\nRequirement already satisfied: fiona&gt;=1.8.19 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from geopandas&gt;=0.12-&gt;esda) (1.9.4)\nRequirement already satisfied: packaging in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from geopandas&gt;=0.12-&gt;esda) (24.1)\nRequirement already satisfied: pyproj&gt;=3.0.1 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from geopandas&gt;=0.12-&gt;esda) (3.6.1)\nRequirement already satisfied: beautifulsoup4&gt;=4.10 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from libpysal&gt;=4.12-&gt;esda) (4.12.2)\nRequirement already satisfied: platformdirs&gt;=2.0.2 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from libpysal&gt;=4.12-&gt;esda) (4.2.2)\nRequirement already satisfied: requests&gt;=2.27 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from libpysal&gt;=4.12-&gt;esda) (2.31.0)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from pandas&gt;1.5-&gt;esda) (2.9.0)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from pandas&gt;1.5-&gt;esda) (2024.1)\nRequirement already satisfied: joblib&gt;=1.1.1 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from scikit-learn&gt;=1.2-&gt;esda) (1.4.2)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from scikit-learn&gt;=1.2-&gt;esda) (3.5.0)\nRequirement already satisfied: soupsieve&gt;1.2 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from beautifulsoup4&gt;=4.10-&gt;libpysal&gt;=4.12-&gt;esda) (2.5)\nRequirement already satisfied: attrs&gt;=19.2.0 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from fiona&gt;=1.8.19-&gt;geopandas&gt;=0.12-&gt;esda) (24.2.0)\nRequirement already satisfied: certifi in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from fiona&gt;=1.8.19-&gt;geopandas&gt;=0.12-&gt;esda) (2024.8.30)\nRequirement already satisfied: click~=8.0 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from fiona&gt;=1.8.19-&gt;geopandas&gt;=0.12-&gt;esda) (8.1.7)\nRequirement already satisfied: click-plugins&gt;=1.0 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from fiona&gt;=1.8.19-&gt;geopandas&gt;=0.12-&gt;esda) (1.1.1)\nRequirement already satisfied: cligj&gt;=0.5 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from fiona&gt;=1.8.19-&gt;geopandas&gt;=0.12-&gt;esda) (0.7.2)\nRequirement already satisfied: six in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from fiona&gt;=1.8.19-&gt;geopandas&gt;=0.12-&gt;esda) (1.16.0)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from requests&gt;=2.27-&gt;libpysal&gt;=4.12-&gt;esda) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from requests&gt;=2.27-&gt;libpysal&gt;=4.12-&gt;esda) (3.8)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages (from requests&gt;=2.27-&gt;libpysal&gt;=4.12-&gt;esda) (1.26.19)\nDownloading esda-2.6.0-py3-none-any.whl (135 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.4/135.4 kB 4.1 MB/s eta 0:00:0000:01\nInstalling collected packages: esda\nSuccessfully installed esda-2.6.0\n\n\n\nw = Queen.from_dataframe(Assault21_net, use_index = False)\ny = Assault21_net['countAssault']\nmoran_loc = Moran_Local(y, w)\n\n\nAssault21_net['LocalMoran'] = moran_loc.Is\nAssault21_net['Cluster'] = moran_loc.q\ncluster_labels = {\n    1: \"HH (High-High)\",\n    2: \"LH (Low-High)\",\n    3: \"LL (Low-Low)\",\n    4: \"HL (High-Low)\"\n}\nAssault21_net['ClusterLabel'] = Assault21_net['Cluster'].map(cluster_labels)\n\n# Plot\nfig, ax = plt.subplots(1, 1, figsize=(10, 10))\nAssault21_net.plot(column='ClusterLabel', categorical=True, legend=True, cmap='Set2', ax=ax)\nplt.title(\"Local Moran's I Clusters\", fontsize=15)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport geopandas as gpd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nfrom geopy.distance import geodesic\nimport numpy as np\n\n#variable_net = variable_net.to_crs(epsg=26971) \n#Assault21 = Assault21.to_crs(epsg=26971)\n\nvariable_net['x'] = variable_net.geometry.x\nvariable_net['y'] = variable_net.geometry.y\nfeatures = variable_net[['x', 'y']]\n\n\nfeatures\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n1.174295e+06\n1.820663e+06\n\n\n78\n1.174301e+06\n1.820385e+06\n\n\n1956\n1.173285e+06\n1.820637e+06\n\n\n2003\n1.173295e+06\n1.821071e+06\n\n\n2033\n1.173968e+06\n1.820657e+06\n\n\n...\n...\n...\n\n\n164967\n1.129189e+06\n1.869197e+06\n\n\n203451\n1.129084e+06\n1.869052e+06\n\n\n175163\n1.201826e+06\n1.819227e+06\n\n\n187824\n1.146707e+06\n1.879604e+06\n\n\n203353\n1.191846e+06\n1.863929e+06\n\n\n\n\n295042 rows × 2 columns\n\n\n\n\n# Step 3: Determine the optimal number of clusters (Optional - Elbow Method)\ninertia = []\nfor k in range(1, 11):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(features)\n    inertia.append(kmeans.inertia_)\n\n/Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\ninertia\n\n[319262043960158.0,\n 109963861030452.19,\n 69086800539043.56,\n 54954279538518.55,\n 44728408740544.78,\n 37266295717974.32,\n 30767713953826.164,\n 27036476429277.887,\n 23996827393807.016,\n 21617617531842.484]\n\n\n\n# Plot Elbow Method\nplt.figure(figsize=(8, 6))\nplt.plot(range(1, 11), inertia, marker='o')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Inertia')\nplt.title('Elbow Method')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Step 4: Fit K-Means with the chosen number of clusters (e.g., k=4)\noptimal_k = 3  # Replace with your chosen k based on the Elbow Method\nkmeans = KMeans(n_clusters=optimal_k, random_state=42)\nvariable_net['cluster'] = kmeans.fit_predict(features)\n\n/Users/emmawit/miniforge3/envs/musa-550-fall-2023/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n# Step 5: Calculate cluster centroids\ncentroids = pd.DataFrame(kmeans.cluster_centers_, columns=['x_centroid', 'y_centroid'])\ncentroids_gdf = gpd.GeoDataFrame(centroids, geometry=gpd.points_from_xy(centroids['x_centroid'], centroids['y_centroid']), crs=variable_net.crs)\n\n\nAssault21 = Assault21[~Assault21.geometry.is_empty].copy()\ncentroids_gdf = centroids_gdf[~centroids_gdf.geometry.is_empty].copy()\n\n# Step 2: Calculate distances using valid geometries\ndef calculate_nearest_cluster_projected(row, centroids):\n    # Calculate Euclidean distances using projected CRS coordinates\n    distances = centroids.apply(lambda c: ((row.geometry.x - c.geometry.x) ** 2 + (row.geometry.y - c.geometry.y) ** 2) ** 0.5, axis=1)\n    return distances.idxmin(), distances.min()\n\n\n# Step 3: Apply the distance calculation\nAssault21[['nearest_cluster', 'distance_to_cluster']] = Assault21.apply(\n    lambda row: calculate_nearest_cluster_projected(row, centroids_gdf), axis=1, result_type='expand'\n)\n\n# Step 4: Analyze results\nassaults_per_cluster = Assault21.groupby('nearest_cluster').size()\nprint(assaults_per_cluster)\n\n# Visualization (unchanged)\nplt.figure(figsize=(10, 8))\nfor cluster_id in range(optimal_k):\n    cluster_points = variable_net[variable_net['cluster'] == cluster_id]\n    plt.scatter(cluster_points['x'], cluster_points['y'], label=f'Cluster {cluster_id}')\n\n#plt.scatter(Assault21.geometry.x, Assault21.geometry.y, color='red', label='Assault Cases', alpha=0.2)\nplt.scatter(centroids_gdf.geometry.x, centroids_gdf.geometry.y, color='black', label='Centroids', marker='X', s=100)\n\nplt.xlabel('X Coordinate (meters)')\nplt.ylabel('Y Coordinate (meters)')\nplt.title('K-Means Clustering with Assault Cases')\nplt.legend()\nplt.show()\n\nnearest_cluster\n0.0    20253\ndtype: int64\n\n\n\n\n\n\n\n\n\n\nimport statsmodels.api as sm\n\nfishnet = fishnet.to_crs(epsg=3435)\n\n\nvariable_net_index = variable_net.drop(columns=['index_right'])\nvariable_net_poly = gpd.sjoin(fishnet, variable_net_index, how='left', predicate='intersects')\n\n/var/folders/_6/7_dyppvx4kl54x9k8qfyyzz80000gn/T/ipykernel_24257/3200189343.py:2: UserWarning: CRS mismatch between the CRS of left geometries and the CRS of right geometries.\nUse `to_crs()` to reproject one of the input geometries to match the CRS of the other.\n\nLeft CRS: EPSG:3435\nRight CRS: EPSG:4326\n\n  variable_net_poly = gpd.sjoin(fishnet, variable_net_index, how='left', predicate='intersects')\n\n\n\n# Aggregate variables by fishnet polygons\n# Summing or averaging predictors as appropriate\nlegend_counts = variable_net_poly.groupby(['uniqueID_left', 'Legend']).size().unstack(fill_value=0)\n\n# Rename columns for clarity\nlegend_counts.columns = ['Graffiti_count', 'StreetLightsOut_count', 'LiquorRetail_count', 'ShotSpotter_count']\n\n# Reset the index to merge with the fishnet\nlegend_counts = legend_counts.reset_index()\n\n# Rename `uniqueID_left` to `uniqueID` for consistency\nlegend_counts.rename(columns={'uniqueID_left': 'uniqueID'}, inplace=True)\n\n# Merge aggregated legend counts back into the fishnet\nvariable_net_agg = fishnet.merge(legend_counts, on='uniqueID', how='left')\n\n# Fill NaNs with 0 (if there are any empty cells)\nvariable_net_agg = variable_net_agg.fillna(0)\n\n\n# Step 2: Merge variable_net and Assault21_net using fishnet\ncombined_net = Assault21_net.merge(variable_net_agg, on='geometry', how='left')\n\n# Fill missing values with 0 for variables with no data in some grids\ncombined_net = combined_net.fillna(0)\n\n\nfeatures = combined_net.drop(columns=['geometry', 'countAssault'])  # Drop target and geometry\nfeatures = features.select_dtypes(include=[np.number])  # Keep only numeric columns\n\n# Ensure the target variable is numeric\ncombined_net['countAssault'] = pd.to_numeric(combined_net['countAssault'], errors='coerce')\ncombined_net['countAssault'] = combined_net['countAssault'].fillna(0)\n\n# Fit the Poisson regression model\nimport statsmodels.api as sm\npoisson_model = sm.GLM(combined_net['countAssault'], features, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n# Print model summary\nprint(poisson_results.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:           countAssault   No. Observations:                 1098\nModel:                            GLM   Df Residuals:                     1090\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -9154.5\nDate:                Thu, 26 Dec 2024   Deviance:                       14158.\nTime:                        15:47:55   Pearson chi2:                 1.91e+04\nNo. Iterations:                    17   Pseudo R-squ. (CS):             0.9996\nCovariance Type:            nonrobust                                         \n=========================================================================================\n                            coef    std err          z      P&gt;|z|      [0.025      0.975]\n-----------------------------------------------------------------------------------------\nuniqueID_x                0.0006   1.17e-05     54.838      0.000       0.001       0.001\ncvID                      0.0188      0.001     35.563      0.000       0.018       0.020\nLocalMoran                0.0565      0.005     10.316      0.000       0.046       0.067\nCluster                   0.0918      0.007     13.469      0.000       0.078       0.105\nuniqueID_y                0.0006   1.17e-05     54.838      0.000       0.001       0.001\nGraffiti_count            0.0011   5.29e-05     21.044      0.000       0.001       0.001\nStreetLightsOut_count     0.1117      0.004     26.912      0.000       0.104       0.120\nLiquorRetail_count        0.0015   2.46e-05     59.255      0.000       0.001       0.002\nShotSpotter_count         0.0329      0.001     54.411      0.000       0.032       0.034\n=========================================================================================\n\n\n\ncoefficients = poisson_results.params\nconf_intervals = poisson_results.conf_int()\nconf_intervals.columns = ['Lower 95%', 'Upper 95%']\n\n# Combine coefficients and confidence intervals into a DataFrame\ncoef_df = pd.DataFrame({\n    'Coefficient': coefficients,\n    'Lower CI': conf_intervals['Lower 95%'],\n    'Upper CI': conf_intervals['Upper 95%']\n})\n\n# Plot\nplt.figure(figsize=(10, 6))\ncoef_df['Coefficient'].plot(kind='bar', yerr=(coef_df['Coefficient'] - coef_df['Lower CI'], \n                                              coef_df['Upper CI'] - coef_df['Coefficient']),\n                            capsize=5, color='skyblue', edgecolor='black')\nplt.axhline(0, color='red', linestyle='--', linewidth=1)\nplt.title('Regression Coefficients with Confidence Intervals')\nplt.ylabel('Coefficient Value')\nplt.xlabel('Variables')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ncombined_net['predicted_assault_count'] = poisson_results.predict(features)\n\n# Plot the predicted counts\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\ncombined_net.plot(column='predicted_assault_count', cmap='OrRd', legend=True, ax=ax,\n                  legend_kwds={'label': \"Predicted Assault Counts\"})\nplt.title(\"Predicted Assault Counts Across the Fishnet\")\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\ncombined_net['residuals'] = combined_net['countAssault'] - combined_net['predicted_assault_count']\n\n# Plot residuals\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\ncombined_net.plot(column='residuals', cmap='coolwarm', legend=True, ax=ax,\n                  legend_kwds={'label': \"Residuals (Observed - Predicted)\"})\nplt.title(\"Residuals of the Model Across the Fishnet\")\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\ncorrelation_matrix = features.corr()\n\n# Plot heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\nplt.title('Correlation Matrix of Predictors')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "AnalysisResults/Untitled.html",
    "href": "AnalysisResults/Untitled.html",
    "title": "Poisson Regression Model",
    "section": "",
    "text": "To analyze the relationship between urban risk factors and the count of assaults, we implemented a Poisson regression model. Below is the step-by-step process undertaken to prepare the data and fit the model:\n\nSpatial Join and Data Aggregation: Using the fishnet grid as the spatial framework, we performed a spatial join between the fishnet and the risk factor dataset (variable_net). Each data point representing a specific risk factor (e.g., graffiti, street light outages, liquor retail stores, and ShotSpotter incidents) was associated with the corresponding grid cell it intersected. This allowed for the aggregation of variables by fishnet polygons, where predictors were either summed or averaged as appropriate.\nRenaming and Cleaning: After aggregation, the resulting columns were renamed for clarity (e.g., Graffiti_count, StreetLightsOut_count, etc.). The index column uniqueID_left was renamed to uniqueID to ensure consistency. Missing values in the dataset were replaced with zeros to handle cells without data.\nMerging Datasets: The aggregated risk factors dataset was merged with the assault dataset (Assault21_net) based on shared geometry. This combined dataset enabled a comprehensive view of all relevant predictors alongside the target variable (countAssault).\nFeature Selection: To prepare the dataset for modeling, the geometry column and the target variable (countAssault) were excluded from the features. Only numeric columns were retained for the analysis, ensuring compatibility with the regression model.\nTarget Variable Cleaning: The target variable (countAssault) was converted to a numeric type, with missing values replaced by zeros. This ensured the data was clean and suitable for the Poisson regression analysis.\nModel Fitting: A Generalized Linear Model (GLM) with a Poisson family was employed to predict the count of assaults (countAssault) using the selected risk factors. The statsmodels library was utilized to fit the model, and the results were summarized to evaluate the significance and impact of the predictors.\n\n\n\nPoisson Model Summary\nThe Poisson regression model analyzes the impact of various predictors on the count of assaults (countAssault) across the spatial grid. Here is the detailed interpretation of the results:\n\nModel Fit and Diagnostics:\n\nNumber of Observations: The model was fitted using 1,098 grid cells.\nLog-Likelihood: The value of -9330.8 suggests the model’s goodness-of-fit. Lower values generally indicate a better fit, but comparisons with other models are necessary for deeper insights.\nDeviance and Pearson Chi-Square: The deviance (14,511) and Pearson chi-square (19,500) indicate the extent of variance explained by the model.\nPseudo R-Squared (Cox & Snell): The value of 0.9994 shows that the model explains a substantial proportion of the variability in the data.\n\nPredictor Significance: All predictors in the model have statistically significant coefficients (p &lt; 0.001), suggesting they are strongly associated with the count of assaults.\nCoefficients: Each coefficient represents the log of the expected count of assaults per unit increase in the predictor, holding other variables constant. A positive coefficient indicates that the predictor increases the assault count, while a negative coefficient would indicate the opposite.\n\n\nUniqueID_x and UniqueID_y: These identifiers have very small coefficients (0.0007), indicating minimal effect on the model.\ncvID (Cross-Validation ID): The coefficient (0.0154) suggests a modest positive relationship with assaults, likely reflecting localized variations captured during cross-validation.\nLocal Moran’s I: The coefficient (0.0609) indicates that areas with higher spatial clustering of similar values have a slight increase in assault counts.\nCluster: The positive coefficient (0.1116) shows that being part of a spatial cluster is associated with a higher assault count.\n\n\nRisk Factors:\n\nGraffiti Count: The coefficient (0.0010) suggests a minimal but positive association between graffiti incidents and assaults.\nStreet Lights Out Count: The coefficient (0.1229) shows that non-functional streetlights are strongly associated with higher assault counts.\nLiquor Retail Count: The coefficient (0.0015) indicates a small but significant association between the presence of liquor retail stores and assaults.\nShotSpotter Count: With a coefficient of 0.0333, incidents detected by ShotSpotter are moderately associated with assault counts.\n\nImplications:\n\nThe strong positive relationships between StreetLightsOut_count, ShotSpotter_count, and assault counts suggest that urban infrastructure and real-time crime detection play critical roles in understanding assault distributions.\nThe association with LiquorRetail_count highlights potential links between alcohol availability and assaults, consistent with existing criminological studies.\nGraffiti, while a minor contributor, might indicate broader socio-environmental conditions affecting assaults.\n\n\n\n\n\n\nCode\nimport statsmodels.api as sm\n\nfishnet = fishnet.to_crs(epsg=3435)\n\n\nvariable_net_index = variable_net.drop(columns=['index_right'])\nvariable_net_poly = gpd.sjoin(fishnet, variable_net_index, how='left', predicate='intersects')\n\nlegend_counts = variable_net_poly.groupby(['uniqueID_left', 'Legend']).size().unstack(fill_value=0)\n\n\nlegend_counts.columns = ['Graffiti_count', 'StreetLightsOut_count', 'LiquorRetail_count', 'ShotSpotter_count']\n\n\nlegend_counts = legend_counts.reset_index()\n\n\nlegend_counts.rename(columns={'uniqueID_left': 'uniqueID'}, inplace=True)\n\n\nvariable_net_agg = fishnet.merge(legend_counts, on='uniqueID', how='left')\n\n\nvariable_net_agg = variable_net_agg.fillna(0)\n\ncombined_net = Assault21_net.merge(variable_net_agg, on='geometry', how='left')\n\ncombined_net = combined_net.fillna(0)\n\nfeatures = combined_net.drop(columns=['geometry', 'countAssault'])  \nfeatures = features.select_dtypes(include=[np.number]) \n\n\ncombined_net['countAssault'] = pd.to_numeric(combined_net['countAssault'], errors='coerce')\ncombined_net['countAssault'] = combined_net['countAssault'].fillna(0)\n\n\nimport statsmodels.api as sm\npoisson_model = sm.GLM(combined_net['countAssault'], features, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n\nprint(poisson_results.summary())\n\n\n\nWe then visualized the regression coefficients and their 95% confidence intervals to better understand the relationships between the predictors and assault counts. The coefficients represent the magnitude and direction of the association for each variable, while the confidence intervals provide a range within which the true coefficient value is likely to fall 95% of the time, indicating the precision of these estimates.\nA bar plot was used to visually compare the coefficients, with error bars illustrating the confidence intervals. Variables with error bars that do not overlap zero are considered statistically significant, as they show a meaningful relationship with the response variable.\nThis visualization provides a quick and intuitive understanding of the predictors’ effects on assault counts. Variables with bars entirely above the zero line (y = 0), such as StreetLightsOut_count and ShotSpotter_count, exhibit a strong positive association, while those with bars overlapping zero suggest a negligible or insignificant effect.\n\n\nCode\ncoefficients = poisson_results.params\nconf_intervals = poisson_results.conf_int()\nconf_intervals.columns = ['Lower 95%', 'Upper 95%']\n\ncoef_df = pd.DataFrame({\n    'Coefficient': coefficients,\n    'Lower CI': conf_intervals['Lower 95%'],\n    'Upper CI': conf_intervals['Upper 95%']\n})\n\nplt.figure(figsize=(10, 6))\ncoef_df['Coefficient'].plot(kind='bar', yerr=(coef_df['Coefficient'] - coef_df['Lower CI'], \n                                              coef_df['Upper CI'] - coef_df['Coefficient']),\n                            capsize=5, color='#d0c7e1', edgecolor='#777181')\nplt.grid(axis='y', linestyle='-', alpha=0.1)\nplt.title('Regression Coefficients with Confidence Intervals')\nplt.ylabel('Coefficient Value')\nplt.xlabel('Variables')\nplt.xticks(rotation=0, ha='center', fontsize=8)\nplt.yticks(fontsize=8)\nplt.gca().set_facecolor('white')\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['left'].set_color('grey')\nplt.gca().spines['bottom'].set_color('grey')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "AnalysisResults/Predicted.html",
    "href": "AnalysisResults/Predicted.html",
    "title": "Predicted Assault Cases",
    "section": "",
    "text": "To extend our analysis, we used the fitted Poisson regression model to predict assault counts across the fishnet grid. This was achieved by applying the predict method from the model to the feature variables. The predicted assault counts were then added as a new column, predicted_assault_count, in the dataset, enabling spatial visualization of the results.\nA choropleth map was created to display the predicted assault counts across the fishnet. The map used a custom color gradient to indicate variations in predicted values, with darker shades representing lower predicted counts.\nThe choropleth map of predicted assault counts reveals notable spatial patterns across Chicago. Specifically, there are at least two distinct hotspots in the southern part of the city where predicted assault counts are significantly higher. Additionally, two potential hotspots are observed in the central area of Chicago, although these appear to be less pronounced compared to those in the south. These hotspots highlight areas with elevated risk levels, suggesting a need for targeted interventions. By identifying these zones, city planners and public safety officials can prioritize resources and develop localized strategies to address the underlying factors contributing to higher assault counts in these regions. This spatial insight underscores the utility of the Poisson regression model in supporting evidence-based decision-making for urban safety and planning initiatives.\n\n\nCode\ncombined_net['predicted_assault_count'] = poisson_results.predict(features)\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 8))\ncombined_net.plot(column='predicted_assault_count', cmap=cmap, legend=False, ax=ax)\nplt.title(\"Predicted Assault Counts (Fishnet)\", fontsize=20, fontweight='bold')\nplt.axis('off')\n\ncax = fig.add_axes([0.15, 0.1, 0.7, 0.01]) # Adjust position and size \ncb = plt.colorbar(plot.get_children()[0], cax=cax, orientation='horizontal') \ncb.set_label(\"Predicted Assault Counts\")\ncb.ax.tick_params(labelsize=8)\n\nplt.show()\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "AnalysisResults/Residuals.html",
    "href": "AnalysisResults/Residuals.html",
    "title": "Residuals",
    "section": "",
    "text": "To assess the model’s performance further, we calculated the residuals by subtracting the predicted assault counts from the observed assault counts for each fishnet cell. The residuals represent the difference between observed and predicted values, highlighting areas where the model overestimates or underestimates assault counts.\nThe residuals were then visualized using a choropleth map, with a color scheme that ranged from dark purple hues to green hues, corresponding to negative and positive residuals, respectively. This map provides a spatial representation of the model’s errors across Chicago. Areas with large positive residuals indicate locations where assaults were underpredicted, whereas large negative residuals point to overpredictions. By examining this map, we can identify patterns or anomalies that may suggest limitations in the model or unaccounted factors influencing assault counts.\nBased on the residuals map, several areas are highlighted in purple and dark purple, indicating locations where the model severely underpredicted assault counts. These underpredictions suggest that the model may be missing key factors or patterns specific to those areas. Conversely, there is one notable area in northern Chicago, highlighted in more vibrant green, where the model overpredicted assault counts. This overprediction may point to either overestimated risk factors or a unique characteristic of the area not adequately captured by the model’s predictors.\n\n\nCode\ncombined_net['residuals'] = combined_net['countAssault'] - combined_net['predicted_assault_count']\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 8))\ncombined_net.plot(column='residuals', cmap=cmap, legend=False, ax=ax)\nplt.title(\"Residuals of the Model (Fishnet)\", fontsize=20, fontweight='bold')\nplt.axis('off')\n\ncax = fig.add_axes([0.15, 0.1, 0.7, 0.01]) # Adjust position and size \ncb = plt.colorbar(plot.get_children()[0], cax=cax, orientation='horizontal') \ncb.set_label(\"Residuals (Observed - Predicted)\")\ncb.ax.tick_params(labelsize=8)\n\nplt.show()\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "AnalysisResults/DistributionofAssaults.html",
    "href": "AnalysisResults/DistributionofAssaults.html",
    "title": "Distribution of Assaults",
    "section": "",
    "text": "The histogram and density plot of assault counts below shows a pronounced right skew, indicating that the majority of areas exhibit low assault counts, while a few areas report significantly higher instances. This distribution suggests that while most neighborhoods experience relatively few assaults, certain locations are disproportionately affected.\n\n\nCode\nif Assault21_net['countAssault'].isna().any():\n    Assault21_net = Assault21_net.dropna()\n\nplt.figure(figsize=(10, 6))\nsns.histplot(Assault21_net['countAssault'], bins=30, color=\"#777181\", edgecolor=\"white\")\nplt.title(\"Distribution of Assaults\", fontsize=18, fontweight='bold')\nplt.suptitle(\"Chicago, IL -- 2021\", fontsize=12, y=0.87)\nplt.xlabel(\"Assault Incidents\", fontsize=10)\nplt.ylabel(\"Count\", fontsize=10)\nplt.xticks(rotation=0, ha='center', fontsize=8)\nplt.yticks(fontsize=8)\nplt.grid(axis='y', linestyle='-', alpha=0.1)\nplt.gca().set_facecolor('white')\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['left'].set_color('grey')\nplt.gca().spines['bottom'].set_color('grey')\nplt.show()\n\n\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=Assault21_net, x='countAssault', fill=True, color='#777181', alpha=0.5, linewidth = 0)\nplt.title(\"Density Plot of Assaults\")\nplt.suptitle(\"Chicago, IL -- 2021\", fontsize=12, y=0.87)\nplt.xlabel(\"Assault Incidents\")\nplt.ylabel(\"Density\")\nplt.xticks(rotation=0, ha='center', fontsize=8)\nplt.yticks(fontsize=8)\nplt.gca().set_facecolor('white')\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['left'].set_color('grey')\nplt.gca().spines['bottom'].set_color('grey')\nplt.show()\n\n\n\n\nTo process and visualize the various risk factors on the fishnet grid, we implemented a systematic approach involving the retrieval, preparation, and integration of multiple datasets. The process began with fetching data from various endpoints using the client.get method. Each endpoint corresponded to a specific urban risk factor, such as graffiti, non-functional street lights, liquor retail stores, and ShotSpotter incidents. The retrieved data was stored as individual DataFrames, filtered to include only records from the year 2018, and further refined to focus on relevant categories, such as graffiti locations and street light outages.\nNext, we prepared the data for geospatial analysis. Each DataFrame was converted into a GeoDataFrame by transforming the latitude and longitude columns into geometric points to ensure proper geographic representation. Additionally, the coordinate reference system (CRS) of each GeoDataFrame was transformed to match that of the fishnet grid, ensuring spatial alignment for subsequent analysis.\nWe added a new column called Legend to each GeoDataFrame, categorizing the data by risk factor. For example, the Legend column included values such as “Graffiti,” “StreetLightsOut,” “LiquorRetail,” and “ShotSpotter,” clearly differentiating between the various datasets. All individual GeoDataFrames were then combined into a single dataset named variable_net using the pd.concat function, creating a unified dataset encompassing all points of interest for the identified risk factors.\nA spatial join (sjoin) was then performed to associate each point representing a specific risk factor with a corresponding fishnet grid cell. This step ensured that every data point was correctly placed within the spatial context of the grid cells, allowing us an examination of the risk factor distribution across the city. Finally, the concatenated DataFrame was converted into a GeoDataFrame with the appropriate CRS retained, allowing for spatial operations and visualization of the risk factors within the grid.\n\n\n\n Back to top"
  },
  {
    "objectID": "AnalysisResults/KMeans.html",
    "href": "AnalysisResults/KMeans.html",
    "title": "K-Means Clustering of Risk Factors",
    "section": "",
    "text": "Following the determination of the optimal number of clusters (3), we applied the K-Means clustering algorithm to partition the dataset into distinct groups based on spatial features. The key steps and results from this clustering analysis are as follows:\n\nFitting K-Means Clustering We first fit the K-Means algorithm to the data using the 3 clusters identified earlier. The algorithm was applied to the spatial coordinates (x, y) of the points, which represent different risk factors (e.g., graffiti, street light outages, liquor retail stores, etc.). The K-Means algorithm groups the data into clusters, with each point assigned to the nearest cluster centroid.\nCalculating Cluster Centroids Once the clusters were formed, we calculated the centroids for each cluster. These centroids represent the mean position of all points within a given cluster. We then converted the centroid coordinates into a GeoDataFrame to enable visualization and further spatial analysis.\nAnalyzing Distance to Cluster Centroids To analyze how assault incidents relate to the clusters, we calculated the Euclidean distance from each assault point to the nearest cluster centroid. This step is essential for understanding the proximity of assault cases to the identified risk clusters.\nCluster Analysis We grouped the assault incidents by the nearest cluster and counted the number of assaults in each cluster. This helped us understand how assault incidents are distributed across the different risk clusters. The result shows the distribution of assaults across the three clusters:\n\n\nCluster 0: 6,089 assaults\nCluster 1: 8,622 assaults\nCluster 2: 5,542 assaults This indicates that assaults are not evenly distributed across the clusters, with Cluster 1 having the highest number of incidents.\n\n\nVisualization Finally, we visualized the K-Means clustering results by plotting the clusters and their centroids on a map. Each cluster is represented by a different color, and the centroids are marked with black “X” symbols. The assault incidents are also visualized in the plot to observe their proximity to the clusters.\n\n\n\nCode\noptimal_k = 3  # Replace with your chosen k based on the Elbow Method\nkmeans = KMeans(n_clusters=optimal_k, random_state=42)\nvariable_net['cluster'] = kmeans.fit_predict(features)\n\n\ncentroids = pd.DataFrame(kmeans.cluster_centers_, columns=['x_centroid', 'y_centroid'])\ncentroids_gdf = gpd.GeoDataFrame(centroids, geometry=gpd.points_from_xy(centroids['x_centroid'], centroids['y_centroid']), crs=variable_net.crs)\n\n\nAssault21 = Assault21[~Assault21.geometry.is_empty].copy()\ncentroids_gdf = centroids_gdf[~centroids_gdf.geometry.is_empty].copy()\n\n# Step 2: Calculate distances using valid geometries\ndef calculate_nearest_cluster_projected(row, centroids):\n    # Calculate Euclidean distances using projected CRS coordinates\n    distances = centroids.apply(lambda c: ((row.geometry.x - c.geometry.x) ** 2 + (row.geometry.y - c.geometry.y) ** 2) ** 0.5, axis=1)\n    return distances.idxmin(), distances.min()\n\n\nAssault21[['nearest_cluster', 'distance_to_cluster']] = Assault21.apply(\n    lambda row: calculate_nearest_cluster_projected(row, centroids_gdf), axis=1, result_type='expand'\n)\n\n# Step 4: Analyze results\nassaults_per_cluster = Assault21.groupby('nearest_cluster').size()\nprint(assaults_per_cluster)\n\n\n\n\nCode\ncustom_colors3 = ['#d2e673', '#d0c7e1', '#777181']  # Add as many colors as clusters\n\n# Visualization\nplt.figure(figsize=(10, 8))\nfor cluster_id, color in zip(range(optimal_k), custom_colors3):\n    cluster_points = variable_net[variable_net['cluster'] == cluster_id]\n    plt.scatter(cluster_points['x'], cluster_points['y'], label=f'Cluster {cluster_id}', color=color)\n\nplt.scatter(centroids_gdf.geometry.x, centroids_gdf.geometry.y, color='#27232e', label='Centroids', marker='X', s=100)\n\nplt.xlabel('X Coordinate (meters)')\nplt.ylabel('Y Coordinate (meters)')\nplt.title('K-Means Clustering with Assault Cases', fontsize=20)\nplt.legend()\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['left'].set_visible(False)\nplt.gca().spines['bottom'].set_visible(False)\nplt.xticks([])\nplt.yticks([])\nplt.show()\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "AnalysisResults/Clustering.html",
    "href": "AnalysisResults/Clustering.html",
    "title": "Clustering Risk Factors: Choosing 3 Clusters",
    "section": "",
    "text": "While the Elbow Method suggested that 2 clusters might be the optimal choice based on the inertia plot, we opted to use 3 clusters for the following key reasons:\n\nAvoiding Underclassification: By selecting 3 clusters, we aim to avoid oversimplifying the data. Limiting the clustering to just two groups might overlook important distinctions within neighborhoods, especially when capturing the full complexity of urban dynamics. Using three clusters allows us to better reflect the diversity present in the dataset, especially in terms of risk factors.\nNuanced Insights: Clustering with 3 groups offers a more detailed exploration of the data. Urban environments like Chicago are marked by diverse socioeconomic, demographic, and spatial characteristics. By incorporating more clusters, we can capture this variability more effectively, providing a more nuanced understanding of crime, socioeconomic conditions, and other risk factors that affect different areas of the city.\nFlexibility in Analysis: Opting for 3 clusters provides more flexibility in subsequent analyses and interpretations. With more clusters, we can identify specific trends or patterns that may be particularly relevant for targeted policy-making or community development. It allows for a more detailed examination of local conditions, which could be overlooked with fewer clusters.\nConsensus Among Indices: Although the Elbow Method and other indices indicated that 2 clusters were the most optimal in terms of inertia reduction, there was still notable support (6 indices) for 3 clusters. This consensus from different indices suggests that three clusters could still be a valid and effective choice for segmenting the risk factors.\nContextual Relevance: Our decision to select 3 clusters was informed by the context of Chicago’s neighborhoods. There are well-documented differences in socioeconomic factors, crime rates, and housing characteristics across different parts of the city. Given this, choosing 3 clusters aligns with our understanding of the urban landscape, where more than two categories might be necessary to capture the full range of neighborhood variations.\nExploratory Nature of Clustering: Clustering is inherently exploratory, and it’s common practice to test multiple cluster numbers to see how the results differ. By choosing 3 clusters, we open the door to explore various groupings, uncovering new patterns or insights that could be important for understanding the spatial distribution of risk factors.\n\n\n\nCode\nimport geopandas as gpd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nfrom geopy.distance import geodesic\nimport numpy as np\n\n#variable_net = variable_net.to_crs(epsg=26971) \n#Assault21 = Assault21.to_crs(epsg=26971)\n\nvariable_net['x'] = variable_net.geometry.x\nvariable_net['y'] = variable_net.geometry.y\nfeatures = variable_net[['x', 'y']]\n\n#features\n\n\n\ninertia = []\nfor k in range(1, 11):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(features)\n    inertia.append(kmeans.inertia_)\n    \n    \n    \n#inertia\n\n\n\n# Plot Elbow Method\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, 11), inertia, marker='o', color='#777181', linewidth = 2.5)\nplt.grid(axis='y', linestyle='-', alpha=0.1)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Inertia')\nplt.title('Elbow Method')\nplt.xticks(rotation=0, ha='center', fontsize=8)\nplt.yticks(fontsize=8)\nplt.gca().set_facecolor('white')\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['left'].set_color('grey')\nplt.gca().spines['bottom'].set_color('grey')\nplt.show()\n\n\n\n\n\n\n Back to top"
  }
]